{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 17490,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002287021154945683,
      "grad_norm": 2.981083393096924,
      "learning_rate": 0.00019978273299028017,
      "loss": 2.3318,
      "step": 20
    },
    {
      "epoch": 0.004574042309891366,
      "grad_norm": 0.7629737257957458,
      "learning_rate": 0.0001995540308747856,
      "loss": 0.7635,
      "step": 40
    },
    {
      "epoch": 0.00686106346483705,
      "grad_norm": 0.6850848197937012,
      "learning_rate": 0.00019932532875929104,
      "loss": 0.7117,
      "step": 60
    },
    {
      "epoch": 0.009148084619782733,
      "grad_norm": 0.8669773936271667,
      "learning_rate": 0.00019909662664379646,
      "loss": 0.6911,
      "step": 80
    },
    {
      "epoch": 0.011435105774728416,
      "grad_norm": 0.5439315438270569,
      "learning_rate": 0.0001988679245283019,
      "loss": 0.6626,
      "step": 100
    },
    {
      "epoch": 0.0137221269296741,
      "grad_norm": 0.6681864857673645,
      "learning_rate": 0.00019863922241280732,
      "loss": 0.6619,
      "step": 120
    },
    {
      "epoch": 0.016009148084619784,
      "grad_norm": 0.5795163512229919,
      "learning_rate": 0.00019841052029731274,
      "loss": 0.6064,
      "step": 140
    },
    {
      "epoch": 0.018296169239565466,
      "grad_norm": 1.0025923252105713,
      "learning_rate": 0.00019818181818181821,
      "loss": 0.6632,
      "step": 160
    },
    {
      "epoch": 0.02058319039451115,
      "grad_norm": 1.0600398778915405,
      "learning_rate": 0.00019795311606632363,
      "loss": 0.6014,
      "step": 180
    },
    {
      "epoch": 0.022870211549456832,
      "grad_norm": 0.6896644234657288,
      "learning_rate": 0.00019772441395082905,
      "loss": 0.6386,
      "step": 200
    },
    {
      "epoch": 0.025157232704402517,
      "grad_norm": 0.6897246837615967,
      "learning_rate": 0.0001974957118353345,
      "loss": 0.5807,
      "step": 220
    },
    {
      "epoch": 0.0274442538593482,
      "grad_norm": 2.2857143878936768,
      "learning_rate": 0.00019726700971983992,
      "loss": 0.6377,
      "step": 240
    },
    {
      "epoch": 0.029731275014293884,
      "grad_norm": 0.7497232556343079,
      "learning_rate": 0.00019703830760434534,
      "loss": 0.5867,
      "step": 260
    },
    {
      "epoch": 0.03201829616923957,
      "grad_norm": 0.6681268215179443,
      "learning_rate": 0.00019680960548885078,
      "loss": 0.5875,
      "step": 280
    },
    {
      "epoch": 0.03430531732418525,
      "grad_norm": 0.7643816471099854,
      "learning_rate": 0.00019658090337335623,
      "loss": 0.5874,
      "step": 300
    },
    {
      "epoch": 0.03659233847913093,
      "grad_norm": 1.0048274993896484,
      "learning_rate": 0.00019635220125786165,
      "loss": 0.5974,
      "step": 320
    },
    {
      "epoch": 0.03887935963407661,
      "grad_norm": 0.5588350892066956,
      "learning_rate": 0.00019612349914236707,
      "loss": 0.5861,
      "step": 340
    },
    {
      "epoch": 0.0411663807890223,
      "grad_norm": 0.7402200102806091,
      "learning_rate": 0.0001958947970268725,
      "loss": 0.5526,
      "step": 360
    },
    {
      "epoch": 0.04345340194396798,
      "grad_norm": 0.9542591571807861,
      "learning_rate": 0.00019566609491137796,
      "loss": 0.5433,
      "step": 380
    },
    {
      "epoch": 0.045740423098913664,
      "grad_norm": 0.6796709895133972,
      "learning_rate": 0.00019543739279588338,
      "loss": 0.5526,
      "step": 400
    },
    {
      "epoch": 0.048027444253859346,
      "grad_norm": 0.6918372511863708,
      "learning_rate": 0.0001952086906803888,
      "loss": 0.5558,
      "step": 420
    },
    {
      "epoch": 0.050314465408805034,
      "grad_norm": 0.9000146985054016,
      "learning_rate": 0.00019497998856489424,
      "loss": 0.5884,
      "step": 440
    },
    {
      "epoch": 0.052601486563750716,
      "grad_norm": 0.4802595376968384,
      "learning_rate": 0.00019475128644939966,
      "loss": 0.5907,
      "step": 460
    },
    {
      "epoch": 0.0548885077186964,
      "grad_norm": 0.659529983997345,
      "learning_rate": 0.00019452258433390508,
      "loss": 0.542,
      "step": 480
    },
    {
      "epoch": 0.05717552887364208,
      "grad_norm": 0.6901577711105347,
      "learning_rate": 0.00019429388221841053,
      "loss": 0.5215,
      "step": 500
    },
    {
      "epoch": 0.05946255002858777,
      "grad_norm": 0.582588791847229,
      "learning_rate": 0.00019406518010291597,
      "loss": 0.4942,
      "step": 520
    },
    {
      "epoch": 0.06174957118353345,
      "grad_norm": 0.6060629487037659,
      "learning_rate": 0.0001938364779874214,
      "loss": 0.5434,
      "step": 540
    },
    {
      "epoch": 0.06403659233847914,
      "grad_norm": 0.7205866575241089,
      "learning_rate": 0.0001936077758719268,
      "loss": 0.5667,
      "step": 560
    },
    {
      "epoch": 0.06632361349342482,
      "grad_norm": 0.5643752813339233,
      "learning_rate": 0.00019337907375643226,
      "loss": 0.5569,
      "step": 580
    },
    {
      "epoch": 0.0686106346483705,
      "grad_norm": 0.6767915487289429,
      "learning_rate": 0.00019315037164093768,
      "loss": 0.5477,
      "step": 600
    },
    {
      "epoch": 0.07089765580331618,
      "grad_norm": 0.5610756874084473,
      "learning_rate": 0.00019292166952544312,
      "loss": 0.5227,
      "step": 620
    },
    {
      "epoch": 0.07318467695826186,
      "grad_norm": 0.6029784679412842,
      "learning_rate": 0.00019269296740994857,
      "loss": 0.6116,
      "step": 640
    },
    {
      "epoch": 0.07547169811320754,
      "grad_norm": 0.5067916512489319,
      "learning_rate": 0.00019246426529445399,
      "loss": 0.5616,
      "step": 660
    },
    {
      "epoch": 0.07775871926815323,
      "grad_norm": 0.6022483706474304,
      "learning_rate": 0.0001922355631789594,
      "loss": 0.5735,
      "step": 680
    },
    {
      "epoch": 0.08004574042309891,
      "grad_norm": 0.528248131275177,
      "learning_rate": 0.00019200686106346485,
      "loss": 0.61,
      "step": 700
    },
    {
      "epoch": 0.0823327615780446,
      "grad_norm": 0.5289815068244934,
      "learning_rate": 0.0001917781589479703,
      "loss": 0.5569,
      "step": 720
    },
    {
      "epoch": 0.08461978273299028,
      "grad_norm": 0.5089497566223145,
      "learning_rate": 0.00019154945683247572,
      "loss": 0.5484,
      "step": 740
    },
    {
      "epoch": 0.08690680388793597,
      "grad_norm": 0.6229516863822937,
      "learning_rate": 0.00019132075471698114,
      "loss": 0.5073,
      "step": 760
    },
    {
      "epoch": 0.08919382504288165,
      "grad_norm": 0.4629531502723694,
      "learning_rate": 0.00019109205260148658,
      "loss": 0.5702,
      "step": 780
    },
    {
      "epoch": 0.09148084619782733,
      "grad_norm": 0.5710512399673462,
      "learning_rate": 0.000190863350485992,
      "loss": 0.5762,
      "step": 800
    },
    {
      "epoch": 0.09376786735277301,
      "grad_norm": 0.5689840316772461,
      "learning_rate": 0.00019063464837049742,
      "loss": 0.5155,
      "step": 820
    },
    {
      "epoch": 0.09605488850771869,
      "grad_norm": 0.6120982766151428,
      "learning_rate": 0.00019040594625500287,
      "loss": 0.5274,
      "step": 840
    },
    {
      "epoch": 0.09834190966266437,
      "grad_norm": 0.6475322842597961,
      "learning_rate": 0.0001901772441395083,
      "loss": 0.5796,
      "step": 860
    },
    {
      "epoch": 0.10062893081761007,
      "grad_norm": 0.6569214463233948,
      "learning_rate": 0.00018994854202401373,
      "loss": 0.5744,
      "step": 880
    },
    {
      "epoch": 0.10291595197255575,
      "grad_norm": 0.7165066599845886,
      "learning_rate": 0.00018971983990851915,
      "loss": 0.5636,
      "step": 900
    },
    {
      "epoch": 0.10520297312750143,
      "grad_norm": 0.785143256187439,
      "learning_rate": 0.0001894911377930246,
      "loss": 0.618,
      "step": 920
    },
    {
      "epoch": 0.10748999428244711,
      "grad_norm": 0.6491080522537231,
      "learning_rate": 0.00018926243567753004,
      "loss": 0.5732,
      "step": 940
    },
    {
      "epoch": 0.1097770154373928,
      "grad_norm": 0.7628817558288574,
      "learning_rate": 0.00018903373356203546,
      "loss": 0.5219,
      "step": 960
    },
    {
      "epoch": 0.11206403659233848,
      "grad_norm": 0.4335481524467468,
      "learning_rate": 0.00018880503144654088,
      "loss": 0.5596,
      "step": 980
    },
    {
      "epoch": 0.11435105774728416,
      "grad_norm": 0.7544282078742981,
      "learning_rate": 0.00018857632933104633,
      "loss": 0.5063,
      "step": 1000
    },
    {
      "epoch": 0.11663807890222985,
      "grad_norm": 0.5487564206123352,
      "learning_rate": 0.00018834762721555174,
      "loss": 0.5076,
      "step": 1020
    },
    {
      "epoch": 0.11892510005717553,
      "grad_norm": 0.5316424369812012,
      "learning_rate": 0.00018811892510005716,
      "loss": 0.5654,
      "step": 1040
    },
    {
      "epoch": 0.12121212121212122,
      "grad_norm": 0.5386747717857361,
      "learning_rate": 0.00018789022298456264,
      "loss": 0.5891,
      "step": 1060
    },
    {
      "epoch": 0.1234991423670669,
      "grad_norm": 0.44796279072761536,
      "learning_rate": 0.00018766152086906806,
      "loss": 0.5374,
      "step": 1080
    },
    {
      "epoch": 0.12578616352201258,
      "grad_norm": 0.5030146837234497,
      "learning_rate": 0.00018743281875357347,
      "loss": 0.5401,
      "step": 1100
    },
    {
      "epoch": 0.12807318467695827,
      "grad_norm": 0.6976617574691772,
      "learning_rate": 0.00018720411663807892,
      "loss": 0.5462,
      "step": 1120
    },
    {
      "epoch": 0.13036020583190394,
      "grad_norm": 0.7341192364692688,
      "learning_rate": 0.00018697541452258434,
      "loss": 0.5335,
      "step": 1140
    },
    {
      "epoch": 0.13264722698684964,
      "grad_norm": 0.5037868022918701,
      "learning_rate": 0.00018674671240708979,
      "loss": 0.5618,
      "step": 1160
    },
    {
      "epoch": 0.1349342481417953,
      "grad_norm": 0.499210387468338,
      "learning_rate": 0.0001865180102915952,
      "loss": 0.5243,
      "step": 1180
    },
    {
      "epoch": 0.137221269296741,
      "grad_norm": 0.5095658898353577,
      "learning_rate": 0.00018628930817610065,
      "loss": 0.5709,
      "step": 1200
    },
    {
      "epoch": 0.13950829045168667,
      "grad_norm": 0.6116339564323425,
      "learning_rate": 0.00018606060606060607,
      "loss": 0.5406,
      "step": 1220
    },
    {
      "epoch": 0.14179531160663236,
      "grad_norm": 0.482517272233963,
      "learning_rate": 0.0001858319039451115,
      "loss": 0.5285,
      "step": 1240
    },
    {
      "epoch": 0.14408233276157806,
      "grad_norm": 0.45480990409851074,
      "learning_rate": 0.00018560320182961693,
      "loss": 0.5353,
      "step": 1260
    },
    {
      "epoch": 0.14636935391652373,
      "grad_norm": 0.5602243542671204,
      "learning_rate": 0.00018537449971412238,
      "loss": 0.5117,
      "step": 1280
    },
    {
      "epoch": 0.14865637507146942,
      "grad_norm": 0.49924522638320923,
      "learning_rate": 0.0001851457975986278,
      "loss": 0.5574,
      "step": 1300
    },
    {
      "epoch": 0.1509433962264151,
      "grad_norm": 0.47786951065063477,
      "learning_rate": 0.00018491709548313322,
      "loss": 0.5297,
      "step": 1320
    },
    {
      "epoch": 0.15323041738136078,
      "grad_norm": 0.666830837726593,
      "learning_rate": 0.00018468839336763866,
      "loss": 0.5472,
      "step": 1340
    },
    {
      "epoch": 0.15551743853630645,
      "grad_norm": 0.6111456155776978,
      "learning_rate": 0.00018445969125214408,
      "loss": 0.5654,
      "step": 1360
    },
    {
      "epoch": 0.15780445969125215,
      "grad_norm": 0.4854995310306549,
      "learning_rate": 0.0001842309891366495,
      "loss": 0.4821,
      "step": 1380
    },
    {
      "epoch": 0.16009148084619781,
      "grad_norm": 0.7433949112892151,
      "learning_rate": 0.00018400228702115495,
      "loss": 0.5332,
      "step": 1400
    },
    {
      "epoch": 0.1623785020011435,
      "grad_norm": 0.5531172752380371,
      "learning_rate": 0.0001837735849056604,
      "loss": 0.5427,
      "step": 1420
    },
    {
      "epoch": 0.1646655231560892,
      "grad_norm": 0.5627330541610718,
      "learning_rate": 0.00018354488279016581,
      "loss": 0.534,
      "step": 1440
    },
    {
      "epoch": 0.16695254431103487,
      "grad_norm": 0.4681602716445923,
      "learning_rate": 0.00018331618067467123,
      "loss": 0.5328,
      "step": 1460
    },
    {
      "epoch": 0.16923956546598057,
      "grad_norm": 0.4510343670845032,
      "learning_rate": 0.00018308747855917668,
      "loss": 0.5383,
      "step": 1480
    },
    {
      "epoch": 0.17152658662092624,
      "grad_norm": 0.4990379512310028,
      "learning_rate": 0.00018285877644368213,
      "loss": 0.5342,
      "step": 1500
    },
    {
      "epoch": 0.17381360777587193,
      "grad_norm": 0.47431913018226624,
      "learning_rate": 0.00018263007432818754,
      "loss": 0.5524,
      "step": 1520
    },
    {
      "epoch": 0.1761006289308176,
      "grad_norm": 0.3373248279094696,
      "learning_rate": 0.000182401372212693,
      "loss": 0.4925,
      "step": 1540
    },
    {
      "epoch": 0.1783876500857633,
      "grad_norm": 0.528290867805481,
      "learning_rate": 0.0001821726700971984,
      "loss": 0.5243,
      "step": 1560
    },
    {
      "epoch": 0.180674671240709,
      "grad_norm": 0.7344843745231628,
      "learning_rate": 0.00018194396798170383,
      "loss": 0.5365,
      "step": 1580
    },
    {
      "epoch": 0.18296169239565466,
      "grad_norm": 0.579441249370575,
      "learning_rate": 0.00018171526586620927,
      "loss": 0.5422,
      "step": 1600
    },
    {
      "epoch": 0.18524871355060035,
      "grad_norm": 0.6881064176559448,
      "learning_rate": 0.00018148656375071472,
      "loss": 0.5337,
      "step": 1620
    },
    {
      "epoch": 0.18753573470554602,
      "grad_norm": 0.4973086416721344,
      "learning_rate": 0.00018125786163522014,
      "loss": 0.5464,
      "step": 1640
    },
    {
      "epoch": 0.18982275586049172,
      "grad_norm": 0.5656041502952576,
      "learning_rate": 0.00018102915951972556,
      "loss": 0.4957,
      "step": 1660
    },
    {
      "epoch": 0.19210977701543738,
      "grad_norm": 0.5822752118110657,
      "learning_rate": 0.000180800457404231,
      "loss": 0.5071,
      "step": 1680
    },
    {
      "epoch": 0.19439679817038308,
      "grad_norm": 0.4596043527126312,
      "learning_rate": 0.00018057175528873642,
      "loss": 0.5084,
      "step": 1700
    },
    {
      "epoch": 0.19668381932532875,
      "grad_norm": 0.5971790552139282,
      "learning_rate": 0.00018034305317324187,
      "loss": 0.5091,
      "step": 1720
    },
    {
      "epoch": 0.19897084048027444,
      "grad_norm": 0.4853529930114746,
      "learning_rate": 0.0001801143510577473,
      "loss": 0.5637,
      "step": 1740
    },
    {
      "epoch": 0.20125786163522014,
      "grad_norm": 0.5815888047218323,
      "learning_rate": 0.00017988564894225273,
      "loss": 0.5381,
      "step": 1760
    },
    {
      "epoch": 0.2035448827901658,
      "grad_norm": 0.4035915434360504,
      "learning_rate": 0.00017965694682675815,
      "loss": 0.5386,
      "step": 1780
    },
    {
      "epoch": 0.2058319039451115,
      "grad_norm": 0.5214104056358337,
      "learning_rate": 0.00017942824471126357,
      "loss": 0.5066,
      "step": 1800
    },
    {
      "epoch": 0.20811892510005717,
      "grad_norm": 0.672936737537384,
      "learning_rate": 0.00017919954259576902,
      "loss": 0.5038,
      "step": 1820
    },
    {
      "epoch": 0.21040594625500286,
      "grad_norm": 0.7788004875183105,
      "learning_rate": 0.00017897084048027446,
      "loss": 0.5904,
      "step": 1840
    },
    {
      "epoch": 0.21269296740994853,
      "grad_norm": 0.46035152673721313,
      "learning_rate": 0.00017874213836477988,
      "loss": 0.5804,
      "step": 1860
    },
    {
      "epoch": 0.21497998856489423,
      "grad_norm": 0.49484962224960327,
      "learning_rate": 0.0001785134362492853,
      "loss": 0.553,
      "step": 1880
    },
    {
      "epoch": 0.21726700971983992,
      "grad_norm": 0.5028408765792847,
      "learning_rate": 0.00017828473413379075,
      "loss": 0.5397,
      "step": 1900
    },
    {
      "epoch": 0.2195540308747856,
      "grad_norm": 0.5005632638931274,
      "learning_rate": 0.00017805603201829617,
      "loss": 0.5214,
      "step": 1920
    },
    {
      "epoch": 0.22184105202973128,
      "grad_norm": 0.453810453414917,
      "learning_rate": 0.0001778273299028016,
      "loss": 0.5294,
      "step": 1940
    },
    {
      "epoch": 0.22412807318467695,
      "grad_norm": 0.564497172832489,
      "learning_rate": 0.00017759862778730706,
      "loss": 0.5249,
      "step": 1960
    },
    {
      "epoch": 0.22641509433962265,
      "grad_norm": 0.552879273891449,
      "learning_rate": 0.00017736992567181248,
      "loss": 0.5325,
      "step": 1980
    },
    {
      "epoch": 0.22870211549456831,
      "grad_norm": 0.6014199256896973,
      "learning_rate": 0.0001771412235563179,
      "loss": 0.5169,
      "step": 2000
    },
    {
      "epoch": 0.230989136649514,
      "grad_norm": 0.9389031529426575,
      "learning_rate": 0.00017691252144082334,
      "loss": 0.548,
      "step": 2020
    },
    {
      "epoch": 0.2332761578044597,
      "grad_norm": 0.6682794690132141,
      "learning_rate": 0.00017668381932532876,
      "loss": 0.5353,
      "step": 2040
    },
    {
      "epoch": 0.23556317895940537,
      "grad_norm": 0.40752196311950684,
      "learning_rate": 0.0001764551172098342,
      "loss": 0.5294,
      "step": 2060
    },
    {
      "epoch": 0.23785020011435107,
      "grad_norm": 0.43561670184135437,
      "learning_rate": 0.00017622641509433963,
      "loss": 0.5086,
      "step": 2080
    },
    {
      "epoch": 0.24013722126929674,
      "grad_norm": 0.6515715718269348,
      "learning_rate": 0.00017599771297884507,
      "loss": 0.5268,
      "step": 2100
    },
    {
      "epoch": 0.24242424242424243,
      "grad_norm": 0.8368759751319885,
      "learning_rate": 0.0001757690108633505,
      "loss": 0.5177,
      "step": 2120
    },
    {
      "epoch": 0.2447112635791881,
      "grad_norm": 0.7409250140190125,
      "learning_rate": 0.0001755403087478559,
      "loss": 0.4781,
      "step": 2140
    },
    {
      "epoch": 0.2469982847341338,
      "grad_norm": 0.45407891273498535,
      "learning_rate": 0.00017531160663236136,
      "loss": 0.5114,
      "step": 2160
    },
    {
      "epoch": 0.24928530588907946,
      "grad_norm": 0.5474618077278137,
      "learning_rate": 0.0001750829045168668,
      "loss": 0.5063,
      "step": 2180
    },
    {
      "epoch": 0.25157232704402516,
      "grad_norm": 0.44145849347114563,
      "learning_rate": 0.00017485420240137222,
      "loss": 0.4988,
      "step": 2200
    },
    {
      "epoch": 0.2538593481989708,
      "grad_norm": 0.5832856297492981,
      "learning_rate": 0.00017462550028587764,
      "loss": 0.5007,
      "step": 2220
    },
    {
      "epoch": 0.25614636935391655,
      "grad_norm": 0.5198095440864563,
      "learning_rate": 0.0001743967981703831,
      "loss": 0.5434,
      "step": 2240
    },
    {
      "epoch": 0.2584333905088622,
      "grad_norm": 0.44152167439460754,
      "learning_rate": 0.0001741680960548885,
      "loss": 0.552,
      "step": 2260
    },
    {
      "epoch": 0.2607204116638079,
      "grad_norm": 0.6237249374389648,
      "learning_rate": 0.00017393939393939395,
      "loss": 0.5407,
      "step": 2280
    },
    {
      "epoch": 0.26300743281875355,
      "grad_norm": 0.6079233288764954,
      "learning_rate": 0.00017371069182389937,
      "loss": 0.5701,
      "step": 2300
    },
    {
      "epoch": 0.2652944539736993,
      "grad_norm": 0.5706355571746826,
      "learning_rate": 0.00017348198970840482,
      "loss": 0.5475,
      "step": 2320
    },
    {
      "epoch": 0.26758147512864494,
      "grad_norm": 0.5635416507720947,
      "learning_rate": 0.00017325328759291024,
      "loss": 0.5326,
      "step": 2340
    },
    {
      "epoch": 0.2698684962835906,
      "grad_norm": 0.7148460149765015,
      "learning_rate": 0.00017302458547741566,
      "loss": 0.5106,
      "step": 2360
    },
    {
      "epoch": 0.27215551743853633,
      "grad_norm": 0.5166585445404053,
      "learning_rate": 0.0001727958833619211,
      "loss": 0.5355,
      "step": 2380
    },
    {
      "epoch": 0.274442538593482,
      "grad_norm": 0.5543807744979858,
      "learning_rate": 0.00017256718124642655,
      "loss": 0.5381,
      "step": 2400
    },
    {
      "epoch": 0.27672955974842767,
      "grad_norm": 0.4937761425971985,
      "learning_rate": 0.00017233847913093197,
      "loss": 0.5203,
      "step": 2420
    },
    {
      "epoch": 0.27901658090337333,
      "grad_norm": 0.6334661841392517,
      "learning_rate": 0.0001721097770154374,
      "loss": 0.5461,
      "step": 2440
    },
    {
      "epoch": 0.28130360205831906,
      "grad_norm": 0.47378677129745483,
      "learning_rate": 0.00017188107489994283,
      "loss": 0.5021,
      "step": 2460
    },
    {
      "epoch": 0.2835906232132647,
      "grad_norm": 0.6890561580657959,
      "learning_rate": 0.00017165237278444825,
      "loss": 0.5571,
      "step": 2480
    },
    {
      "epoch": 0.2858776443682104,
      "grad_norm": 0.48307567834854126,
      "learning_rate": 0.0001714236706689537,
      "loss": 0.4958,
      "step": 2500
    },
    {
      "epoch": 0.2881646655231561,
      "grad_norm": 0.4741389751434326,
      "learning_rate": 0.00017119496855345914,
      "loss": 0.509,
      "step": 2520
    },
    {
      "epoch": 0.2904516866781018,
      "grad_norm": 0.4896453022956848,
      "learning_rate": 0.00017096626643796456,
      "loss": 0.5271,
      "step": 2540
    },
    {
      "epoch": 0.29273870783304745,
      "grad_norm": 0.45103752613067627,
      "learning_rate": 0.00017073756432246998,
      "loss": 0.5598,
      "step": 2560
    },
    {
      "epoch": 0.2950257289879931,
      "grad_norm": 0.4488523602485657,
      "learning_rate": 0.00017050886220697543,
      "loss": 0.5539,
      "step": 2580
    },
    {
      "epoch": 0.29731275014293884,
      "grad_norm": 0.48567140102386475,
      "learning_rate": 0.00017028016009148085,
      "loss": 0.5214,
      "step": 2600
    },
    {
      "epoch": 0.2995997712978845,
      "grad_norm": 0.6442090272903442,
      "learning_rate": 0.0001700514579759863,
      "loss": 0.539,
      "step": 2620
    },
    {
      "epoch": 0.3018867924528302,
      "grad_norm": 0.5704731345176697,
      "learning_rate": 0.0001698227558604917,
      "loss": 0.5293,
      "step": 2640
    },
    {
      "epoch": 0.30417381360777584,
      "grad_norm": 0.7595179677009583,
      "learning_rate": 0.00016959405374499716,
      "loss": 0.5147,
      "step": 2660
    },
    {
      "epoch": 0.30646083476272157,
      "grad_norm": 0.5179930329322815,
      "learning_rate": 0.00016936535162950258,
      "loss": 0.4821,
      "step": 2680
    },
    {
      "epoch": 0.30874785591766724,
      "grad_norm": 0.3631264567375183,
      "learning_rate": 0.000169136649514008,
      "loss": 0.4954,
      "step": 2700
    },
    {
      "epoch": 0.3110348770726129,
      "grad_norm": 0.6834476590156555,
      "learning_rate": 0.00016890794739851347,
      "loss": 0.5096,
      "step": 2720
    },
    {
      "epoch": 0.3133218982275586,
      "grad_norm": 0.5700468420982361,
      "learning_rate": 0.0001686792452830189,
      "loss": 0.5065,
      "step": 2740
    },
    {
      "epoch": 0.3156089193825043,
      "grad_norm": 0.42175403237342834,
      "learning_rate": 0.0001684505431675243,
      "loss": 0.5085,
      "step": 2760
    },
    {
      "epoch": 0.31789594053744996,
      "grad_norm": 0.6031057834625244,
      "learning_rate": 0.00016822184105202973,
      "loss": 0.5341,
      "step": 2780
    },
    {
      "epoch": 0.32018296169239563,
      "grad_norm": 0.42975929379463196,
      "learning_rate": 0.00016799313893653517,
      "loss": 0.4654,
      "step": 2800
    },
    {
      "epoch": 0.32246998284734135,
      "grad_norm": 0.4932768940925598,
      "learning_rate": 0.0001677644368210406,
      "loss": 0.5755,
      "step": 2820
    },
    {
      "epoch": 0.324757004002287,
      "grad_norm": 0.4869372546672821,
      "learning_rate": 0.00016753573470554604,
      "loss": 0.5504,
      "step": 2840
    },
    {
      "epoch": 0.3270440251572327,
      "grad_norm": 0.5075709819793701,
      "learning_rate": 0.00016730703259005148,
      "loss": 0.5496,
      "step": 2860
    },
    {
      "epoch": 0.3293310463121784,
      "grad_norm": 0.5227895379066467,
      "learning_rate": 0.0001670783304745569,
      "loss": 0.5708,
      "step": 2880
    },
    {
      "epoch": 0.3316180674671241,
      "grad_norm": 0.5992273688316345,
      "learning_rate": 0.00016684962835906232,
      "loss": 0.4981,
      "step": 2900
    },
    {
      "epoch": 0.33390508862206975,
      "grad_norm": 0.6103905439376831,
      "learning_rate": 0.00016662092624356777,
      "loss": 0.5376,
      "step": 2920
    },
    {
      "epoch": 0.3361921097770154,
      "grad_norm": 0.4735010862350464,
      "learning_rate": 0.00016639222412807319,
      "loss": 0.5379,
      "step": 2940
    },
    {
      "epoch": 0.33847913093196114,
      "grad_norm": 0.6044975519180298,
      "learning_rate": 0.00016616352201257863,
      "loss": 0.5314,
      "step": 2960
    },
    {
      "epoch": 0.3407661520869068,
      "grad_norm": 0.39195483922958374,
      "learning_rate": 0.00016593481989708405,
      "loss": 0.558,
      "step": 2980
    },
    {
      "epoch": 0.34305317324185247,
      "grad_norm": 0.44853535294532776,
      "learning_rate": 0.0001657061177815895,
      "loss": 0.5827,
      "step": 3000
    },
    {
      "epoch": 0.3453401943967982,
      "grad_norm": 0.518858015537262,
      "learning_rate": 0.00016547741566609492,
      "loss": 0.5236,
      "step": 3020
    },
    {
      "epoch": 0.34762721555174386,
      "grad_norm": 0.45724937319755554,
      "learning_rate": 0.00016524871355060033,
      "loss": 0.5512,
      "step": 3040
    },
    {
      "epoch": 0.34991423670668953,
      "grad_norm": 0.7700961232185364,
      "learning_rate": 0.00016502001143510578,
      "loss": 0.522,
      "step": 3060
    },
    {
      "epoch": 0.3522012578616352,
      "grad_norm": 0.4803321957588196,
      "learning_rate": 0.00016479130931961123,
      "loss": 0.5091,
      "step": 3080
    },
    {
      "epoch": 0.3544882790165809,
      "grad_norm": 0.5467798113822937,
      "learning_rate": 0.00016456260720411665,
      "loss": 0.5403,
      "step": 3100
    },
    {
      "epoch": 0.3567753001715266,
      "grad_norm": 0.4723912179470062,
      "learning_rate": 0.00016433390508862206,
      "loss": 0.5086,
      "step": 3120
    },
    {
      "epoch": 0.35906232132647226,
      "grad_norm": 0.47227999567985535,
      "learning_rate": 0.0001641052029731275,
      "loss": 0.513,
      "step": 3140
    },
    {
      "epoch": 0.361349342481418,
      "grad_norm": 0.543400764465332,
      "learning_rate": 0.00016387650085763293,
      "loss": 0.5311,
      "step": 3160
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 0.506062388420105,
      "learning_rate": 0.00016364779874213838,
      "loss": 0.5052,
      "step": 3180
    },
    {
      "epoch": 0.3659233847913093,
      "grad_norm": 0.4963044226169586,
      "learning_rate": 0.00016341909662664382,
      "loss": 0.5217,
      "step": 3200
    },
    {
      "epoch": 0.368210405946255,
      "grad_norm": 0.6311402320861816,
      "learning_rate": 0.00016319039451114924,
      "loss": 0.5141,
      "step": 3220
    },
    {
      "epoch": 0.3704974271012007,
      "grad_norm": 0.4985625743865967,
      "learning_rate": 0.00016296169239565466,
      "loss": 0.5394,
      "step": 3240
    },
    {
      "epoch": 0.3727844482561464,
      "grad_norm": 0.5049694776535034,
      "learning_rate": 0.00016273299028016008,
      "loss": 0.5208,
      "step": 3260
    },
    {
      "epoch": 0.37507146941109204,
      "grad_norm": 0.5702717900276184,
      "learning_rate": 0.00016250428816466555,
      "loss": 0.5549,
      "step": 3280
    },
    {
      "epoch": 0.37735849056603776,
      "grad_norm": 0.5529832243919373,
      "learning_rate": 0.00016227558604917097,
      "loss": 0.5529,
      "step": 3300
    },
    {
      "epoch": 0.37964551172098343,
      "grad_norm": 0.6509438753128052,
      "learning_rate": 0.0001620468839336764,
      "loss": 0.5772,
      "step": 3320
    },
    {
      "epoch": 0.3819325328759291,
      "grad_norm": 0.5600323677062988,
      "learning_rate": 0.00016181818181818184,
      "loss": 0.5387,
      "step": 3340
    },
    {
      "epoch": 0.38421955403087477,
      "grad_norm": 0.5956742167472839,
      "learning_rate": 0.00016158947970268726,
      "loss": 0.5609,
      "step": 3360
    },
    {
      "epoch": 0.3865065751858205,
      "grad_norm": 0.6175770163536072,
      "learning_rate": 0.00016136077758719267,
      "loss": 0.4812,
      "step": 3380
    },
    {
      "epoch": 0.38879359634076616,
      "grad_norm": 0.4483315646648407,
      "learning_rate": 0.00016113207547169812,
      "loss": 0.4407,
      "step": 3400
    },
    {
      "epoch": 0.3910806174957118,
      "grad_norm": 0.8093593716621399,
      "learning_rate": 0.00016090337335620357,
      "loss": 0.5225,
      "step": 3420
    },
    {
      "epoch": 0.3933676386506575,
      "grad_norm": 0.6412184834480286,
      "learning_rate": 0.00016067467124070899,
      "loss": 0.536,
      "step": 3440
    },
    {
      "epoch": 0.3956546598056032,
      "grad_norm": 0.6750466227531433,
      "learning_rate": 0.0001604459691252144,
      "loss": 0.4797,
      "step": 3460
    },
    {
      "epoch": 0.3979416809605489,
      "grad_norm": 0.45482996106147766,
      "learning_rate": 0.00016021726700971985,
      "loss": 0.4939,
      "step": 3480
    },
    {
      "epoch": 0.40022870211549455,
      "grad_norm": 0.3954762816429138,
      "learning_rate": 0.0001599885648942253,
      "loss": 0.4498,
      "step": 3500
    },
    {
      "epoch": 0.4025157232704403,
      "grad_norm": 0.6013856530189514,
      "learning_rate": 0.00015975986277873072,
      "loss": 0.4947,
      "step": 3520
    },
    {
      "epoch": 0.40480274442538594,
      "grad_norm": 0.46771764755249023,
      "learning_rate": 0.00015953116066323613,
      "loss": 0.5396,
      "step": 3540
    },
    {
      "epoch": 0.4070897655803316,
      "grad_norm": 0.6017089486122131,
      "learning_rate": 0.00015930245854774158,
      "loss": 0.4919,
      "step": 3560
    },
    {
      "epoch": 0.4093767867352773,
      "grad_norm": 0.41865891218185425,
      "learning_rate": 0.000159073756432247,
      "loss": 0.4974,
      "step": 3580
    },
    {
      "epoch": 0.411663807890223,
      "grad_norm": 0.4366830289363861,
      "learning_rate": 0.00015884505431675242,
      "loss": 0.539,
      "step": 3600
    },
    {
      "epoch": 0.41395082904516867,
      "grad_norm": 0.35338106751441956,
      "learning_rate": 0.0001586163522012579,
      "loss": 0.4549,
      "step": 3620
    },
    {
      "epoch": 0.41623785020011433,
      "grad_norm": 0.9945013523101807,
      "learning_rate": 0.0001583876500857633,
      "loss": 0.4941,
      "step": 3640
    },
    {
      "epoch": 0.41852487135506006,
      "grad_norm": 0.5036702156066895,
      "learning_rate": 0.00015815894797026873,
      "loss": 0.5341,
      "step": 3660
    },
    {
      "epoch": 0.4208118925100057,
      "grad_norm": 1.0279955863952637,
      "learning_rate": 0.00015793024585477415,
      "loss": 0.5054,
      "step": 3680
    },
    {
      "epoch": 0.4230989136649514,
      "grad_norm": 0.5288063287734985,
      "learning_rate": 0.0001577015437392796,
      "loss": 0.5049,
      "step": 3700
    },
    {
      "epoch": 0.42538593481989706,
      "grad_norm": 0.5357201099395752,
      "learning_rate": 0.000157472841623785,
      "loss": 0.5159,
      "step": 3720
    },
    {
      "epoch": 0.4276729559748428,
      "grad_norm": 0.7484234571456909,
      "learning_rate": 0.00015724413950829046,
      "loss": 0.5313,
      "step": 3740
    },
    {
      "epoch": 0.42995997712978845,
      "grad_norm": 0.6485673189163208,
      "learning_rate": 0.0001570154373927959,
      "loss": 0.4941,
      "step": 3760
    },
    {
      "epoch": 0.4322469982847341,
      "grad_norm": 0.46604838967323303,
      "learning_rate": 0.00015678673527730132,
      "loss": 0.4715,
      "step": 3780
    },
    {
      "epoch": 0.43453401943967984,
      "grad_norm": 0.4796045422554016,
      "learning_rate": 0.00015655803316180674,
      "loss": 0.5367,
      "step": 3800
    },
    {
      "epoch": 0.4368210405946255,
      "grad_norm": 0.3383714258670807,
      "learning_rate": 0.0001563293310463122,
      "loss": 0.5219,
      "step": 3820
    },
    {
      "epoch": 0.4391080617495712,
      "grad_norm": 0.47331827878952026,
      "learning_rate": 0.00015610062893081764,
      "loss": 0.4933,
      "step": 3840
    },
    {
      "epoch": 0.44139508290451684,
      "grad_norm": 0.6572577357292175,
      "learning_rate": 0.00015587192681532305,
      "loss": 0.5371,
      "step": 3860
    },
    {
      "epoch": 0.44368210405946257,
      "grad_norm": 0.7039676904678345,
      "learning_rate": 0.00015564322469982847,
      "loss": 0.5396,
      "step": 3880
    },
    {
      "epoch": 0.44596912521440824,
      "grad_norm": 0.6977074146270752,
      "learning_rate": 0.00015541452258433392,
      "loss": 0.5063,
      "step": 3900
    },
    {
      "epoch": 0.4482561463693539,
      "grad_norm": 0.5812172889709473,
      "learning_rate": 0.00015518582046883934,
      "loss": 0.5529,
      "step": 3920
    },
    {
      "epoch": 0.4505431675242996,
      "grad_norm": 0.4889400899410248,
      "learning_rate": 0.00015495711835334476,
      "loss": 0.477,
      "step": 3940
    },
    {
      "epoch": 0.4528301886792453,
      "grad_norm": 0.49832648038864136,
      "learning_rate": 0.0001547284162378502,
      "loss": 0.514,
      "step": 3960
    },
    {
      "epoch": 0.45511720983419096,
      "grad_norm": 0.5995912551879883,
      "learning_rate": 0.00015449971412235565,
      "loss": 0.4844,
      "step": 3980
    },
    {
      "epoch": 0.45740423098913663,
      "grad_norm": 0.5935076475143433,
      "learning_rate": 0.00015427101200686107,
      "loss": 0.5114,
      "step": 4000
    },
    {
      "epoch": 0.45969125214408235,
      "grad_norm": 0.5112745761871338,
      "learning_rate": 0.0001540423098913665,
      "loss": 0.5216,
      "step": 4020
    },
    {
      "epoch": 0.461978273299028,
      "grad_norm": 0.6113353967666626,
      "learning_rate": 0.00015381360777587193,
      "loss": 0.5127,
      "step": 4040
    },
    {
      "epoch": 0.4642652944539737,
      "grad_norm": 0.5359131693840027,
      "learning_rate": 0.00015358490566037738,
      "loss": 0.4878,
      "step": 4060
    },
    {
      "epoch": 0.4665523156089194,
      "grad_norm": 0.5316219925880432,
      "learning_rate": 0.0001533562035448828,
      "loss": 0.4978,
      "step": 4080
    },
    {
      "epoch": 0.4688393367638651,
      "grad_norm": 0.4364728331565857,
      "learning_rate": 0.00015312750142938824,
      "loss": 0.5596,
      "step": 4100
    },
    {
      "epoch": 0.47112635791881075,
      "grad_norm": 0.43722954392433167,
      "learning_rate": 0.00015289879931389366,
      "loss": 0.5224,
      "step": 4120
    },
    {
      "epoch": 0.4734133790737564,
      "grad_norm": 0.5115541219711304,
      "learning_rate": 0.00015267009719839908,
      "loss": 0.5503,
      "step": 4140
    },
    {
      "epoch": 0.47570040022870214,
      "grad_norm": 0.5387282371520996,
      "learning_rate": 0.0001524413950829045,
      "loss": 0.5109,
      "step": 4160
    },
    {
      "epoch": 0.4779874213836478,
      "grad_norm": 0.5896246433258057,
      "learning_rate": 0.00015221269296740997,
      "loss": 0.5236,
      "step": 4180
    },
    {
      "epoch": 0.48027444253859347,
      "grad_norm": 0.4902034103870392,
      "learning_rate": 0.0001519839908519154,
      "loss": 0.479,
      "step": 4200
    },
    {
      "epoch": 0.48256146369353914,
      "grad_norm": 0.7434343695640564,
      "learning_rate": 0.0001517552887364208,
      "loss": 0.4916,
      "step": 4220
    },
    {
      "epoch": 0.48484848484848486,
      "grad_norm": 0.8214027285575867,
      "learning_rate": 0.00015152658662092626,
      "loss": 0.5261,
      "step": 4240
    },
    {
      "epoch": 0.48713550600343053,
      "grad_norm": 0.46775591373443604,
      "learning_rate": 0.00015129788450543168,
      "loss": 0.5108,
      "step": 4260
    },
    {
      "epoch": 0.4894225271583762,
      "grad_norm": 0.48166409134864807,
      "learning_rate": 0.00015106918238993712,
      "loss": 0.5453,
      "step": 4280
    },
    {
      "epoch": 0.4917095483133219,
      "grad_norm": 0.5116118788719177,
      "learning_rate": 0.00015084048027444254,
      "loss": 0.5065,
      "step": 4300
    },
    {
      "epoch": 0.4939965694682676,
      "grad_norm": 0.5612274408340454,
      "learning_rate": 0.000150611778158948,
      "loss": 0.531,
      "step": 4320
    },
    {
      "epoch": 0.49628359062321326,
      "grad_norm": 0.7324530482292175,
      "learning_rate": 0.0001503830760434534,
      "loss": 0.5085,
      "step": 4340
    },
    {
      "epoch": 0.4985706117781589,
      "grad_norm": 0.5729169845581055,
      "learning_rate": 0.00015015437392795883,
      "loss": 0.5546,
      "step": 4360
    },
    {
      "epoch": 0.5008576329331046,
      "grad_norm": 0.43678590655326843,
      "learning_rate": 0.00014992567181246427,
      "loss": 0.5044,
      "step": 4380
    },
    {
      "epoch": 0.5031446540880503,
      "grad_norm": 0.4122704565525055,
      "learning_rate": 0.00014969696969696972,
      "loss": 0.5261,
      "step": 4400
    },
    {
      "epoch": 0.505431675242996,
      "grad_norm": 0.47516754269599915,
      "learning_rate": 0.00014946826758147514,
      "loss": 0.5124,
      "step": 4420
    },
    {
      "epoch": 0.5077186963979416,
      "grad_norm": 0.6699530482292175,
      "learning_rate": 0.00014923956546598056,
      "loss": 0.4944,
      "step": 4440
    },
    {
      "epoch": 0.5100057175528874,
      "grad_norm": 0.5583884119987488,
      "learning_rate": 0.000149010863350486,
      "loss": 0.4924,
      "step": 4460
    },
    {
      "epoch": 0.5122927387078331,
      "grad_norm": 0.4979620575904846,
      "learning_rate": 0.00014878216123499142,
      "loss": 0.491,
      "step": 4480
    },
    {
      "epoch": 0.5145797598627787,
      "grad_norm": 0.6616359949111938,
      "learning_rate": 0.00014855345911949684,
      "loss": 0.527,
      "step": 4500
    },
    {
      "epoch": 0.5168667810177244,
      "grad_norm": 0.6467901468276978,
      "learning_rate": 0.00014832475700400231,
      "loss": 0.5315,
      "step": 4520
    },
    {
      "epoch": 0.51915380217267,
      "grad_norm": 0.4873522222042084,
      "learning_rate": 0.00014809605488850773,
      "loss": 0.4917,
      "step": 4540
    },
    {
      "epoch": 0.5214408233276158,
      "grad_norm": 0.50728440284729,
      "learning_rate": 0.00014786735277301315,
      "loss": 0.4736,
      "step": 4560
    },
    {
      "epoch": 0.5237278444825615,
      "grad_norm": 0.5333706736564636,
      "learning_rate": 0.00014763865065751857,
      "loss": 0.5519,
      "step": 4580
    },
    {
      "epoch": 0.5260148656375071,
      "grad_norm": 0.7114156484603882,
      "learning_rate": 0.00014740994854202402,
      "loss": 0.5463,
      "step": 4600
    },
    {
      "epoch": 0.5283018867924528,
      "grad_norm": 0.40294453501701355,
      "learning_rate": 0.00014718124642652946,
      "loss": 0.4929,
      "step": 4620
    },
    {
      "epoch": 0.5305889079473985,
      "grad_norm": 0.5837861895561218,
      "learning_rate": 0.00014695254431103488,
      "loss": 0.5317,
      "step": 4640
    },
    {
      "epoch": 0.5328759291023442,
      "grad_norm": 0.5498517155647278,
      "learning_rate": 0.00014672384219554033,
      "loss": 0.5009,
      "step": 4660
    },
    {
      "epoch": 0.5351629502572899,
      "grad_norm": 0.5259889960289001,
      "learning_rate": 0.00014649514008004575,
      "loss": 0.5142,
      "step": 4680
    },
    {
      "epoch": 0.5374499714122356,
      "grad_norm": 0.5968292355537415,
      "learning_rate": 0.00014626643796455117,
      "loss": 0.5374,
      "step": 4700
    },
    {
      "epoch": 0.5397369925671812,
      "grad_norm": 0.6221994161605835,
      "learning_rate": 0.0001460377358490566,
      "loss": 0.5546,
      "step": 4720
    },
    {
      "epoch": 0.5420240137221269,
      "grad_norm": 0.4419800937175751,
      "learning_rate": 0.00014580903373356206,
      "loss": 0.5512,
      "step": 4740
    },
    {
      "epoch": 0.5443110348770727,
      "grad_norm": 0.6554311513900757,
      "learning_rate": 0.00014558033161806748,
      "loss": 0.5136,
      "step": 4760
    },
    {
      "epoch": 0.5465980560320183,
      "grad_norm": 0.5018590688705444,
      "learning_rate": 0.0001453516295025729,
      "loss": 0.4931,
      "step": 4780
    },
    {
      "epoch": 0.548885077186964,
      "grad_norm": 0.5251941680908203,
      "learning_rate": 0.00014512292738707834,
      "loss": 0.517,
      "step": 4800
    },
    {
      "epoch": 0.5511720983419096,
      "grad_norm": 0.6166695952415466,
      "learning_rate": 0.00014489422527158376,
      "loss": 0.5139,
      "step": 4820
    },
    {
      "epoch": 0.5534591194968553,
      "grad_norm": 0.5570288300514221,
      "learning_rate": 0.0001446655231560892,
      "loss": 0.4843,
      "step": 4840
    },
    {
      "epoch": 0.5557461406518011,
      "grad_norm": 0.5750868916511536,
      "learning_rate": 0.00014443682104059463,
      "loss": 0.5328,
      "step": 4860
    },
    {
      "epoch": 0.5580331618067467,
      "grad_norm": 0.5337663888931274,
      "learning_rate": 0.00014420811892510007,
      "loss": 0.491,
      "step": 4880
    },
    {
      "epoch": 0.5603201829616924,
      "grad_norm": 0.5166962742805481,
      "learning_rate": 0.0001439794168096055,
      "loss": 0.5543,
      "step": 4900
    },
    {
      "epoch": 0.5626072041166381,
      "grad_norm": 0.485500305891037,
      "learning_rate": 0.0001437507146941109,
      "loss": 0.5626,
      "step": 4920
    },
    {
      "epoch": 0.5648942252715837,
      "grad_norm": 0.4628584384918213,
      "learning_rate": 0.00014352201257861636,
      "loss": 0.4423,
      "step": 4940
    },
    {
      "epoch": 0.5671812464265295,
      "grad_norm": 0.6021524667739868,
      "learning_rate": 0.0001432933104631218,
      "loss": 0.5411,
      "step": 4960
    },
    {
      "epoch": 0.5694682675814752,
      "grad_norm": 0.49345558881759644,
      "learning_rate": 0.00014306460834762722,
      "loss": 0.5604,
      "step": 4980
    },
    {
      "epoch": 0.5717552887364208,
      "grad_norm": 0.480094850063324,
      "learning_rate": 0.00014283590623213267,
      "loss": 0.4632,
      "step": 5000
    },
    {
      "epoch": 0.5740423098913665,
      "grad_norm": 0.5439661741256714,
      "learning_rate": 0.0001426072041166381,
      "loss": 0.4988,
      "step": 5020
    },
    {
      "epoch": 0.5763293310463122,
      "grad_norm": 0.5975801944732666,
      "learning_rate": 0.0001423785020011435,
      "loss": 0.4993,
      "step": 5040
    },
    {
      "epoch": 0.5786163522012578,
      "grad_norm": 0.5089402198791504,
      "learning_rate": 0.00014214979988564895,
      "loss": 0.503,
      "step": 5060
    },
    {
      "epoch": 0.5809033733562036,
      "grad_norm": 0.49133360385894775,
      "learning_rate": 0.0001419210977701544,
      "loss": 0.5541,
      "step": 5080
    },
    {
      "epoch": 0.5831903945111492,
      "grad_norm": 0.5648404359817505,
      "learning_rate": 0.00014169239565465982,
      "loss": 0.5291,
      "step": 5100
    },
    {
      "epoch": 0.5854774156660949,
      "grad_norm": 0.4744011461734772,
      "learning_rate": 0.00014146369353916524,
      "loss": 0.5088,
      "step": 5120
    },
    {
      "epoch": 0.5877644368210406,
      "grad_norm": 0.5634410381317139,
      "learning_rate": 0.00014123499142367068,
      "loss": 0.52,
      "step": 5140
    },
    {
      "epoch": 0.5900514579759862,
      "grad_norm": 0.6475132703781128,
      "learning_rate": 0.0001410062893081761,
      "loss": 0.5566,
      "step": 5160
    },
    {
      "epoch": 0.592338479130932,
      "grad_norm": 0.5197082161903381,
      "learning_rate": 0.00014077758719268155,
      "loss": 0.5404,
      "step": 5180
    },
    {
      "epoch": 0.5946255002858777,
      "grad_norm": 0.6486137509346008,
      "learning_rate": 0.00014054888507718697,
      "loss": 0.5566,
      "step": 5200
    },
    {
      "epoch": 0.5969125214408233,
      "grad_norm": 0.5662815570831299,
      "learning_rate": 0.0001403201829616924,
      "loss": 0.5385,
      "step": 5220
    },
    {
      "epoch": 0.599199542595769,
      "grad_norm": 0.5292709469795227,
      "learning_rate": 0.00014009148084619783,
      "loss": 0.5424,
      "step": 5240
    },
    {
      "epoch": 0.6014865637507147,
      "grad_norm": 0.5354471802711487,
      "learning_rate": 0.00013986277873070325,
      "loss": 0.5191,
      "step": 5260
    },
    {
      "epoch": 0.6037735849056604,
      "grad_norm": 0.7268193960189819,
      "learning_rate": 0.0001396340766152087,
      "loss": 0.4765,
      "step": 5280
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 0.793743908405304,
      "learning_rate": 0.00013940537449971414,
      "loss": 0.4921,
      "step": 5300
    },
    {
      "epoch": 0.6083476272155517,
      "grad_norm": 0.7005276679992676,
      "learning_rate": 0.00013917667238421956,
      "loss": 0.527,
      "step": 5320
    },
    {
      "epoch": 0.6106346483704974,
      "grad_norm": 0.5376774072647095,
      "learning_rate": 0.00013894797026872498,
      "loss": 0.5143,
      "step": 5340
    },
    {
      "epoch": 0.6129216695254431,
      "grad_norm": 0.7076200246810913,
      "learning_rate": 0.00013871926815323043,
      "loss": 0.5411,
      "step": 5360
    },
    {
      "epoch": 0.6152086906803887,
      "grad_norm": 0.7326474785804749,
      "learning_rate": 0.00013849056603773585,
      "loss": 0.4918,
      "step": 5380
    },
    {
      "epoch": 0.6174957118353345,
      "grad_norm": 0.7879929542541504,
      "learning_rate": 0.0001382618639222413,
      "loss": 0.5334,
      "step": 5400
    },
    {
      "epoch": 0.6197827329902802,
      "grad_norm": 0.4518359303474426,
      "learning_rate": 0.00013803316180674674,
      "loss": 0.4888,
      "step": 5420
    },
    {
      "epoch": 0.6220697541452258,
      "grad_norm": 0.5475798845291138,
      "learning_rate": 0.00013780445969125216,
      "loss": 0.517,
      "step": 5440
    },
    {
      "epoch": 0.6243567753001715,
      "grad_norm": 0.5309435725212097,
      "learning_rate": 0.00013757575757575758,
      "loss": 0.5174,
      "step": 5460
    },
    {
      "epoch": 0.6266437964551173,
      "grad_norm": 0.7024722695350647,
      "learning_rate": 0.00013734705546026302,
      "loss": 0.4793,
      "step": 5480
    },
    {
      "epoch": 0.6289308176100629,
      "grad_norm": 0.6274563670158386,
      "learning_rate": 0.00013711835334476844,
      "loss": 0.4916,
      "step": 5500
    },
    {
      "epoch": 0.6312178387650086,
      "grad_norm": 0.7409972548484802,
      "learning_rate": 0.00013688965122927389,
      "loss": 0.5025,
      "step": 5520
    },
    {
      "epoch": 0.6335048599199543,
      "grad_norm": 0.6031084060668945,
      "learning_rate": 0.0001366609491137793,
      "loss": 0.522,
      "step": 5540
    },
    {
      "epoch": 0.6357918810748999,
      "grad_norm": 0.7779771685600281,
      "learning_rate": 0.00013643224699828475,
      "loss": 0.4721,
      "step": 5560
    },
    {
      "epoch": 0.6380789022298456,
      "grad_norm": 0.458359032869339,
      "learning_rate": 0.00013620354488279017,
      "loss": 0.5267,
      "step": 5580
    },
    {
      "epoch": 0.6403659233847913,
      "grad_norm": 0.4277102053165436,
      "learning_rate": 0.0001359748427672956,
      "loss": 0.529,
      "step": 5600
    },
    {
      "epoch": 0.642652944539737,
      "grad_norm": 0.5292364358901978,
      "learning_rate": 0.00013574614065180104,
      "loss": 0.5134,
      "step": 5620
    },
    {
      "epoch": 0.6449399656946827,
      "grad_norm": 0.5943016409873962,
      "learning_rate": 0.00013551743853630648,
      "loss": 0.5523,
      "step": 5640
    },
    {
      "epoch": 0.6472269868496283,
      "grad_norm": 0.6075900197029114,
      "learning_rate": 0.0001352887364208119,
      "loss": 0.4964,
      "step": 5660
    },
    {
      "epoch": 0.649514008004574,
      "grad_norm": 0.5931020975112915,
      "learning_rate": 0.00013506003430531732,
      "loss": 0.4932,
      "step": 5680
    },
    {
      "epoch": 0.6518010291595198,
      "grad_norm": 0.46845582127571106,
      "learning_rate": 0.00013483133218982277,
      "loss": 0.4614,
      "step": 5700
    },
    {
      "epoch": 0.6540880503144654,
      "grad_norm": 0.528680682182312,
      "learning_rate": 0.00013460263007432818,
      "loss": 0.5029,
      "step": 5720
    },
    {
      "epoch": 0.6563750714694111,
      "grad_norm": 0.43856680393218994,
      "learning_rate": 0.00013437392795883363,
      "loss": 0.5392,
      "step": 5740
    },
    {
      "epoch": 0.6586620926243568,
      "grad_norm": 0.4476870000362396,
      "learning_rate": 0.00013414522584333905,
      "loss": 0.5238,
      "step": 5760
    },
    {
      "epoch": 0.6609491137793024,
      "grad_norm": 0.7062170505523682,
      "learning_rate": 0.0001339165237278445,
      "loss": 0.4991,
      "step": 5780
    },
    {
      "epoch": 0.6632361349342482,
      "grad_norm": 0.5927556157112122,
      "learning_rate": 0.00013368782161234991,
      "loss": 0.5055,
      "step": 5800
    },
    {
      "epoch": 0.6655231560891939,
      "grad_norm": 0.5492741465568542,
      "learning_rate": 0.00013345911949685533,
      "loss": 0.4957,
      "step": 5820
    },
    {
      "epoch": 0.6678101772441395,
      "grad_norm": 0.5577230453491211,
      "learning_rate": 0.0001332304173813608,
      "loss": 0.4942,
      "step": 5840
    },
    {
      "epoch": 0.6700971983990852,
      "grad_norm": 0.47607120871543884,
      "learning_rate": 0.00013300171526586623,
      "loss": 0.5354,
      "step": 5860
    },
    {
      "epoch": 0.6723842195540308,
      "grad_norm": 0.4730030298233032,
      "learning_rate": 0.00013277301315037164,
      "loss": 0.5569,
      "step": 5880
    },
    {
      "epoch": 0.6746712407089765,
      "grad_norm": 0.4570251703262329,
      "learning_rate": 0.0001325443110348771,
      "loss": 0.5104,
      "step": 5900
    },
    {
      "epoch": 0.6769582618639223,
      "grad_norm": 0.5062201619148254,
      "learning_rate": 0.0001323156089193825,
      "loss": 0.5228,
      "step": 5920
    },
    {
      "epoch": 0.6792452830188679,
      "grad_norm": 0.5326629877090454,
      "learning_rate": 0.00013208690680388793,
      "loss": 0.5202,
      "step": 5940
    },
    {
      "epoch": 0.6815323041738136,
      "grad_norm": 0.5645000338554382,
      "learning_rate": 0.00013185820468839337,
      "loss": 0.4846,
      "step": 5960
    },
    {
      "epoch": 0.6838193253287593,
      "grad_norm": 0.8561466932296753,
      "learning_rate": 0.00013162950257289882,
      "loss": 0.5121,
      "step": 5980
    },
    {
      "epoch": 0.6861063464837049,
      "grad_norm": 0.4421786963939667,
      "learning_rate": 0.00013140080045740424,
      "loss": 0.4831,
      "step": 6000
    },
    {
      "epoch": 0.6883933676386507,
      "grad_norm": 0.6397421956062317,
      "learning_rate": 0.00013117209834190966,
      "loss": 0.5421,
      "step": 6020
    },
    {
      "epoch": 0.6906803887935964,
      "grad_norm": 0.5983035564422607,
      "learning_rate": 0.0001309433962264151,
      "loss": 0.4706,
      "step": 6040
    },
    {
      "epoch": 0.692967409948542,
      "grad_norm": 0.5114258527755737,
      "learning_rate": 0.00013071469411092052,
      "loss": 0.5051,
      "step": 6060
    },
    {
      "epoch": 0.6952544311034877,
      "grad_norm": 0.5354829430580139,
      "learning_rate": 0.00013048599199542597,
      "loss": 0.4975,
      "step": 6080
    },
    {
      "epoch": 0.6975414522584333,
      "grad_norm": 0.7259959578514099,
      "learning_rate": 0.0001302572898799314,
      "loss": 0.5009,
      "step": 6100
    },
    {
      "epoch": 0.6998284734133791,
      "grad_norm": 0.48397254943847656,
      "learning_rate": 0.00013002858776443683,
      "loss": 0.5005,
      "step": 6120
    },
    {
      "epoch": 0.7021154945683248,
      "grad_norm": 0.6573075652122498,
      "learning_rate": 0.00012979988564894225,
      "loss": 0.5206,
      "step": 6140
    },
    {
      "epoch": 0.7044025157232704,
      "grad_norm": 0.5602856874465942,
      "learning_rate": 0.00012957118353344767,
      "loss": 0.4643,
      "step": 6160
    },
    {
      "epoch": 0.7066895368782161,
      "grad_norm": 0.49915561079978943,
      "learning_rate": 0.00012934248141795312,
      "loss": 0.5344,
      "step": 6180
    },
    {
      "epoch": 0.7089765580331618,
      "grad_norm": 0.5021153688430786,
      "learning_rate": 0.00012911377930245857,
      "loss": 0.4939,
      "step": 6200
    },
    {
      "epoch": 0.7112635791881075,
      "grad_norm": 0.537609875202179,
      "learning_rate": 0.00012888507718696398,
      "loss": 0.4607,
      "step": 6220
    },
    {
      "epoch": 0.7135506003430532,
      "grad_norm": 0.5667852163314819,
      "learning_rate": 0.0001286563750714694,
      "loss": 0.4838,
      "step": 6240
    },
    {
      "epoch": 0.7158376214979989,
      "grad_norm": 0.8837944865226746,
      "learning_rate": 0.00012842767295597485,
      "loss": 0.5188,
      "step": 6260
    },
    {
      "epoch": 0.7181246426529445,
      "grad_norm": 0.5309888124465942,
      "learning_rate": 0.00012819897084048027,
      "loss": 0.5253,
      "step": 6280
    },
    {
      "epoch": 0.7204116638078902,
      "grad_norm": 0.6151757836341858,
      "learning_rate": 0.00012797026872498571,
      "loss": 0.5178,
      "step": 6300
    },
    {
      "epoch": 0.722698684962836,
      "grad_norm": 0.5700852870941162,
      "learning_rate": 0.00012774156660949116,
      "loss": 0.4642,
      "step": 6320
    },
    {
      "epoch": 0.7249857061177816,
      "grad_norm": 0.4576015770435333,
      "learning_rate": 0.00012751286449399658,
      "loss": 0.4768,
      "step": 6340
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 0.6028570532798767,
      "learning_rate": 0.000127284162378502,
      "loss": 0.4883,
      "step": 6360
    },
    {
      "epoch": 0.7295597484276729,
      "grad_norm": 0.548956036567688,
      "learning_rate": 0.00012705546026300744,
      "loss": 0.4888,
      "step": 6380
    },
    {
      "epoch": 0.7318467695826186,
      "grad_norm": 0.5059283375740051,
      "learning_rate": 0.0001268267581475129,
      "loss": 0.494,
      "step": 6400
    },
    {
      "epoch": 0.7341337907375644,
      "grad_norm": 0.5589597821235657,
      "learning_rate": 0.0001265980560320183,
      "loss": 0.5236,
      "step": 6420
    },
    {
      "epoch": 0.73642081189251,
      "grad_norm": 0.6594955921173096,
      "learning_rate": 0.00012636935391652373,
      "loss": 0.529,
      "step": 6440
    },
    {
      "epoch": 0.7387078330474557,
      "grad_norm": 0.6687647700309753,
      "learning_rate": 0.00012614065180102917,
      "loss": 0.4547,
      "step": 6460
    },
    {
      "epoch": 0.7409948542024014,
      "grad_norm": 0.618057370185852,
      "learning_rate": 0.0001259119496855346,
      "loss": 0.4756,
      "step": 6480
    },
    {
      "epoch": 0.743281875357347,
      "grad_norm": 0.6286019086837769,
      "learning_rate": 0.00012568324757004,
      "loss": 0.511,
      "step": 6500
    },
    {
      "epoch": 0.7455688965122927,
      "grad_norm": 0.6099529266357422,
      "learning_rate": 0.00012545454545454546,
      "loss": 0.4732,
      "step": 6520
    },
    {
      "epoch": 0.7478559176672385,
      "grad_norm": 0.5673983097076416,
      "learning_rate": 0.0001252258433390509,
      "loss": 0.5007,
      "step": 6540
    },
    {
      "epoch": 0.7501429388221841,
      "grad_norm": 0.5433483123779297,
      "learning_rate": 0.00012499714122355632,
      "loss": 0.5081,
      "step": 6560
    },
    {
      "epoch": 0.7524299599771298,
      "grad_norm": 0.5238233804702759,
      "learning_rate": 0.00012476843910806174,
      "loss": 0.5106,
      "step": 6580
    },
    {
      "epoch": 0.7547169811320755,
      "grad_norm": 0.7196140289306641,
      "learning_rate": 0.0001245397369925672,
      "loss": 0.4935,
      "step": 6600
    },
    {
      "epoch": 0.7570040022870211,
      "grad_norm": 0.5177969932556152,
      "learning_rate": 0.00012431103487707263,
      "loss": 0.461,
      "step": 6620
    },
    {
      "epoch": 0.7592910234419669,
      "grad_norm": 0.4917968511581421,
      "learning_rate": 0.00012408233276157805,
      "loss": 0.4963,
      "step": 6640
    },
    {
      "epoch": 0.7615780445969125,
      "grad_norm": 0.6231672167778015,
      "learning_rate": 0.00012385363064608347,
      "loss": 0.5156,
      "step": 6660
    },
    {
      "epoch": 0.7638650657518582,
      "grad_norm": 0.5347297191619873,
      "learning_rate": 0.00012362492853058892,
      "loss": 0.5125,
      "step": 6680
    },
    {
      "epoch": 0.7661520869068039,
      "grad_norm": 0.47168076038360596,
      "learning_rate": 0.00012339622641509434,
      "loss": 0.5359,
      "step": 6700
    },
    {
      "epoch": 0.7684391080617495,
      "grad_norm": 0.5463703274726868,
      "learning_rate": 0.00012316752429959976,
      "loss": 0.4849,
      "step": 6720
    },
    {
      "epoch": 0.7707261292166953,
      "grad_norm": 0.5126420259475708,
      "learning_rate": 0.00012293882218410523,
      "loss": 0.4817,
      "step": 6740
    },
    {
      "epoch": 0.773013150371641,
      "grad_norm": 0.39109548926353455,
      "learning_rate": 0.00012271012006861065,
      "loss": 0.5146,
      "step": 6760
    },
    {
      "epoch": 0.7753001715265866,
      "grad_norm": 0.5405777096748352,
      "learning_rate": 0.00012248141795311607,
      "loss": 0.5011,
      "step": 6780
    },
    {
      "epoch": 0.7775871926815323,
      "grad_norm": 0.5399099588394165,
      "learning_rate": 0.00012225271583762151,
      "loss": 0.4848,
      "step": 6800
    },
    {
      "epoch": 0.779874213836478,
      "grad_norm": 0.45064178109169006,
      "learning_rate": 0.00012202401372212693,
      "loss": 0.5138,
      "step": 6820
    },
    {
      "epoch": 0.7821612349914236,
      "grad_norm": 0.6843581795692444,
      "learning_rate": 0.00012179531160663235,
      "loss": 0.4639,
      "step": 6840
    },
    {
      "epoch": 0.7844482561463694,
      "grad_norm": 0.5798754096031189,
      "learning_rate": 0.00012156660949113781,
      "loss": 0.5476,
      "step": 6860
    },
    {
      "epoch": 0.786735277301315,
      "grad_norm": 0.5818547010421753,
      "learning_rate": 0.00012133790737564323,
      "loss": 0.4502,
      "step": 6880
    },
    {
      "epoch": 0.7890222984562607,
      "grad_norm": 0.7064045667648315,
      "learning_rate": 0.00012110920526014866,
      "loss": 0.5134,
      "step": 6900
    },
    {
      "epoch": 0.7913093196112064,
      "grad_norm": 0.6303932070732117,
      "learning_rate": 0.0001208805031446541,
      "loss": 0.4864,
      "step": 6920
    },
    {
      "epoch": 0.793596340766152,
      "grad_norm": 0.490234375,
      "learning_rate": 0.00012065180102915951,
      "loss": 0.4646,
      "step": 6940
    },
    {
      "epoch": 0.7958833619210978,
      "grad_norm": 0.6060947179794312,
      "learning_rate": 0.00012042309891366496,
      "loss": 0.4994,
      "step": 6960
    },
    {
      "epoch": 0.7981703830760435,
      "grad_norm": 0.6294294595718384,
      "learning_rate": 0.00012019439679817039,
      "loss": 0.497,
      "step": 6980
    },
    {
      "epoch": 0.8004574042309891,
      "grad_norm": 0.47724202275276184,
      "learning_rate": 0.00011996569468267583,
      "loss": 0.5562,
      "step": 7000
    },
    {
      "epoch": 0.8027444253859348,
      "grad_norm": 0.507236123085022,
      "learning_rate": 0.00011973699256718124,
      "loss": 0.5269,
      "step": 7020
    },
    {
      "epoch": 0.8050314465408805,
      "grad_norm": 0.5488964915275574,
      "learning_rate": 0.00011950829045168668,
      "loss": 0.497,
      "step": 7040
    },
    {
      "epoch": 0.8073184676958262,
      "grad_norm": 0.511341392993927,
      "learning_rate": 0.00011927958833619211,
      "loss": 0.4699,
      "step": 7060
    },
    {
      "epoch": 0.8096054888507719,
      "grad_norm": 0.6991346478462219,
      "learning_rate": 0.00011905088622069756,
      "loss": 0.5109,
      "step": 7080
    },
    {
      "epoch": 0.8118925100057176,
      "grad_norm": 0.5792605876922607,
      "learning_rate": 0.00011882218410520299,
      "loss": 0.5416,
      "step": 7100
    },
    {
      "epoch": 0.8141795311606632,
      "grad_norm": 0.43330898880958557,
      "learning_rate": 0.00011859348198970841,
      "loss": 0.4474,
      "step": 7120
    },
    {
      "epoch": 0.8164665523156089,
      "grad_norm": 0.5833532214164734,
      "learning_rate": 0.00011836477987421384,
      "loss": 0.4929,
      "step": 7140
    },
    {
      "epoch": 0.8187535734705546,
      "grad_norm": 0.6653885245323181,
      "learning_rate": 0.00011813607775871927,
      "loss": 0.4736,
      "step": 7160
    },
    {
      "epoch": 0.8210405946255003,
      "grad_norm": 0.8263478875160217,
      "learning_rate": 0.00011790737564322472,
      "loss": 0.491,
      "step": 7180
    },
    {
      "epoch": 0.823327615780446,
      "grad_norm": 0.4632074534893036,
      "learning_rate": 0.00011767867352773014,
      "loss": 0.5059,
      "step": 7200
    },
    {
      "epoch": 0.8256146369353916,
      "grad_norm": 0.6802851557731628,
      "learning_rate": 0.00011744997141223557,
      "loss": 0.4832,
      "step": 7220
    },
    {
      "epoch": 0.8279016580903373,
      "grad_norm": 0.7097399234771729,
      "learning_rate": 0.000117221269296741,
      "loss": 0.4912,
      "step": 7240
    },
    {
      "epoch": 0.8301886792452831,
      "grad_norm": 0.6823658347129822,
      "learning_rate": 0.00011699256718124642,
      "loss": 0.5117,
      "step": 7260
    },
    {
      "epoch": 0.8324757004002287,
      "grad_norm": 0.5333234667778015,
      "learning_rate": 0.00011676386506575185,
      "loss": 0.5315,
      "step": 7280
    },
    {
      "epoch": 0.8347627215551744,
      "grad_norm": 0.5759730339050293,
      "learning_rate": 0.0001165351629502573,
      "loss": 0.5051,
      "step": 7300
    },
    {
      "epoch": 0.8370497427101201,
      "grad_norm": 0.6923638582229614,
      "learning_rate": 0.00011630646083476273,
      "loss": 0.4841,
      "step": 7320
    },
    {
      "epoch": 0.8393367638650657,
      "grad_norm": 0.4934523403644562,
      "learning_rate": 0.00011607775871926816,
      "loss": 0.4564,
      "step": 7340
    },
    {
      "epoch": 0.8416237850200115,
      "grad_norm": 0.556079626083374,
      "learning_rate": 0.00011584905660377358,
      "loss": 0.4357,
      "step": 7360
    },
    {
      "epoch": 0.8439108061749572,
      "grad_norm": 0.5461217164993286,
      "learning_rate": 0.00011562035448827902,
      "loss": 0.4784,
      "step": 7380
    },
    {
      "epoch": 0.8461978273299028,
      "grad_norm": 0.5439979434013367,
      "learning_rate": 0.00011539165237278446,
      "loss": 0.5361,
      "step": 7400
    },
    {
      "epoch": 0.8484848484848485,
      "grad_norm": 0.5035380125045776,
      "learning_rate": 0.0001151629502572899,
      "loss": 0.4725,
      "step": 7420
    },
    {
      "epoch": 0.8507718696397941,
      "grad_norm": 0.6959767937660217,
      "learning_rate": 0.00011493424814179531,
      "loss": 0.4814,
      "step": 7440
    },
    {
      "epoch": 0.8530588907947398,
      "grad_norm": 0.5715444087982178,
      "learning_rate": 0.00011470554602630075,
      "loss": 0.4877,
      "step": 7460
    },
    {
      "epoch": 0.8553459119496856,
      "grad_norm": 0.5117819905281067,
      "learning_rate": 0.00011447684391080618,
      "loss": 0.4995,
      "step": 7480
    },
    {
      "epoch": 0.8576329331046312,
      "grad_norm": 0.419776052236557,
      "learning_rate": 0.0001142481417953116,
      "loss": 0.4925,
      "step": 7500
    },
    {
      "epoch": 0.8599199542595769,
      "grad_norm": 0.6358798146247864,
      "learning_rate": 0.00011401943967981706,
      "loss": 0.4521,
      "step": 7520
    },
    {
      "epoch": 0.8622069754145226,
      "grad_norm": 0.4669320285320282,
      "learning_rate": 0.00011379073756432248,
      "loss": 0.4928,
      "step": 7540
    },
    {
      "epoch": 0.8644939965694682,
      "grad_norm": 0.45929399132728577,
      "learning_rate": 0.00011356203544882791,
      "loss": 0.4992,
      "step": 7560
    },
    {
      "epoch": 0.866781017724414,
      "grad_norm": 0.49980852007865906,
      "learning_rate": 0.00011333333333333334,
      "loss": 0.5454,
      "step": 7580
    },
    {
      "epoch": 0.8690680388793597,
      "grad_norm": 0.6060739159584045,
      "learning_rate": 0.00011310463121783876,
      "loss": 0.4988,
      "step": 7600
    },
    {
      "epoch": 0.8713550600343053,
      "grad_norm": 0.37560611963272095,
      "learning_rate": 0.00011287592910234419,
      "loss": 0.4952,
      "step": 7620
    },
    {
      "epoch": 0.873642081189251,
      "grad_norm": 0.5040997266769409,
      "learning_rate": 0.00011264722698684964,
      "loss": 0.5294,
      "step": 7640
    },
    {
      "epoch": 0.8759291023441966,
      "grad_norm": 0.45590969920158386,
      "learning_rate": 0.00011241852487135507,
      "loss": 0.5329,
      "step": 7660
    },
    {
      "epoch": 0.8782161234991424,
      "grad_norm": 0.3901595175266266,
      "learning_rate": 0.00011218982275586049,
      "loss": 0.5114,
      "step": 7680
    },
    {
      "epoch": 0.8805031446540881,
      "grad_norm": 0.4346764087677002,
      "learning_rate": 0.00011196112064036592,
      "loss": 0.4758,
      "step": 7700
    },
    {
      "epoch": 0.8827901658090337,
      "grad_norm": 0.3952285051345825,
      "learning_rate": 0.00011173241852487136,
      "loss": 0.4795,
      "step": 7720
    },
    {
      "epoch": 0.8850771869639794,
      "grad_norm": 0.5314043760299683,
      "learning_rate": 0.0001115037164093768,
      "loss": 0.4739,
      "step": 7740
    },
    {
      "epoch": 0.8873642081189251,
      "grad_norm": 0.456891804933548,
      "learning_rate": 0.00011127501429388223,
      "loss": 0.534,
      "step": 7760
    },
    {
      "epoch": 0.8896512292738707,
      "grad_norm": 0.7742429971694946,
      "learning_rate": 0.00011104631217838765,
      "loss": 0.5137,
      "step": 7780
    },
    {
      "epoch": 0.8919382504288165,
      "grad_norm": 0.5712659955024719,
      "learning_rate": 0.00011081761006289309,
      "loss": 0.4992,
      "step": 7800
    },
    {
      "epoch": 0.8942252715837622,
      "grad_norm": 0.5827536582946777,
      "learning_rate": 0.00011058890794739852,
      "loss": 0.4733,
      "step": 7820
    },
    {
      "epoch": 0.8965122927387078,
      "grad_norm": 0.5404132008552551,
      "learning_rate": 0.00011036020583190394,
      "loss": 0.5137,
      "step": 7840
    },
    {
      "epoch": 0.8987993138936535,
      "grad_norm": 0.6305522918701172,
      "learning_rate": 0.00011013150371640938,
      "loss": 0.492,
      "step": 7860
    },
    {
      "epoch": 0.9010863350485993,
      "grad_norm": 0.7782062292098999,
      "learning_rate": 0.00010990280160091482,
      "loss": 0.5086,
      "step": 7880
    },
    {
      "epoch": 0.9033733562035449,
      "grad_norm": 0.5285969376564026,
      "learning_rate": 0.00010967409948542025,
      "loss": 0.4726,
      "step": 7900
    },
    {
      "epoch": 0.9056603773584906,
      "grad_norm": 0.6543861627578735,
      "learning_rate": 0.00010944539736992567,
      "loss": 0.4719,
      "step": 7920
    },
    {
      "epoch": 0.9079473985134362,
      "grad_norm": 0.5441111922264099,
      "learning_rate": 0.0001092166952544311,
      "loss": 0.4616,
      "step": 7940
    },
    {
      "epoch": 0.9102344196683819,
      "grad_norm": 0.5130698680877686,
      "learning_rate": 0.00010898799313893655,
      "loss": 0.4823,
      "step": 7960
    },
    {
      "epoch": 0.9125214408233276,
      "grad_norm": 0.6331362128257751,
      "learning_rate": 0.00010875929102344198,
      "loss": 0.4967,
      "step": 7980
    },
    {
      "epoch": 0.9148084619782733,
      "grad_norm": 0.6986405253410339,
      "learning_rate": 0.00010853058890794741,
      "loss": 0.5113,
      "step": 8000
    },
    {
      "epoch": 0.917095483133219,
      "grad_norm": 0.514892041683197,
      "learning_rate": 0.00010830188679245283,
      "loss": 0.4544,
      "step": 8020
    },
    {
      "epoch": 0.9193825042881647,
      "grad_norm": 0.5293344855308533,
      "learning_rate": 0.00010807318467695826,
      "loss": 0.5112,
      "step": 8040
    },
    {
      "epoch": 0.9216695254431103,
      "grad_norm": 0.5163779258728027,
      "learning_rate": 0.0001078444825614637,
      "loss": 0.5136,
      "step": 8060
    },
    {
      "epoch": 0.923956546598056,
      "grad_norm": 0.5721810460090637,
      "learning_rate": 0.00010761578044596914,
      "loss": 0.4925,
      "step": 8080
    },
    {
      "epoch": 0.9262435677530018,
      "grad_norm": 0.49838927388191223,
      "learning_rate": 0.00010738707833047456,
      "loss": 0.4952,
      "step": 8100
    },
    {
      "epoch": 0.9285305889079474,
      "grad_norm": 0.39261674880981445,
      "learning_rate": 0.00010715837621497999,
      "loss": 0.5104,
      "step": 8120
    },
    {
      "epoch": 0.9308176100628931,
      "grad_norm": 0.6772252917289734,
      "learning_rate": 0.00010692967409948543,
      "loss": 0.5115,
      "step": 8140
    },
    {
      "epoch": 0.9331046312178388,
      "grad_norm": 0.7387341260910034,
      "learning_rate": 0.00010670097198399084,
      "loss": 0.5345,
      "step": 8160
    },
    {
      "epoch": 0.9353916523727844,
      "grad_norm": 0.4193933606147766,
      "learning_rate": 0.0001064722698684963,
      "loss": 0.4998,
      "step": 8180
    },
    {
      "epoch": 0.9376786735277302,
      "grad_norm": 0.8004553318023682,
      "learning_rate": 0.00010624356775300172,
      "loss": 0.5277,
      "step": 8200
    },
    {
      "epoch": 0.9399656946826758,
      "grad_norm": 0.5979626178741455,
      "learning_rate": 0.00010601486563750716,
      "loss": 0.4626,
      "step": 8220
    },
    {
      "epoch": 0.9422527158376215,
      "grad_norm": 0.6165457963943481,
      "learning_rate": 0.00010578616352201259,
      "loss": 0.4971,
      "step": 8240
    },
    {
      "epoch": 0.9445397369925672,
      "grad_norm": 0.6335148811340332,
      "learning_rate": 0.000105557461406518,
      "loss": 0.5248,
      "step": 8260
    },
    {
      "epoch": 0.9468267581475128,
      "grad_norm": 0.6566277742385864,
      "learning_rate": 0.00010532875929102344,
      "loss": 0.5524,
      "step": 8280
    },
    {
      "epoch": 0.9491137793024585,
      "grad_norm": 0.5937104821205139,
      "learning_rate": 0.00010510005717552889,
      "loss": 0.5244,
      "step": 8300
    },
    {
      "epoch": 0.9514008004574043,
      "grad_norm": 0.5447213053703308,
      "learning_rate": 0.00010487135506003432,
      "loss": 0.5139,
      "step": 8320
    },
    {
      "epoch": 0.9536878216123499,
      "grad_norm": 0.4847105145454407,
      "learning_rate": 0.00010464265294453974,
      "loss": 0.4757,
      "step": 8340
    },
    {
      "epoch": 0.9559748427672956,
      "grad_norm": 0.6264209151268005,
      "learning_rate": 0.00010441395082904517,
      "loss": 0.4745,
      "step": 8360
    },
    {
      "epoch": 0.9582618639222413,
      "grad_norm": 0.5349419713020325,
      "learning_rate": 0.0001041852487135506,
      "loss": 0.5189,
      "step": 8380
    },
    {
      "epoch": 0.9605488850771869,
      "grad_norm": 0.4760887324810028,
      "learning_rate": 0.00010395654659805602,
      "loss": 0.5163,
      "step": 8400
    },
    {
      "epoch": 0.9628359062321327,
      "grad_norm": 0.5314644575119019,
      "learning_rate": 0.00010372784448256148,
      "loss": 0.4912,
      "step": 8420
    },
    {
      "epoch": 0.9651229273870783,
      "grad_norm": 0.6038137078285217,
      "learning_rate": 0.0001034991423670669,
      "loss": 0.5105,
      "step": 8440
    },
    {
      "epoch": 0.967409948542024,
      "grad_norm": 0.5294288992881775,
      "learning_rate": 0.00010327044025157233,
      "loss": 0.4606,
      "step": 8460
    },
    {
      "epoch": 0.9696969696969697,
      "grad_norm": 0.48125624656677246,
      "learning_rate": 0.00010304173813607776,
      "loss": 0.5147,
      "step": 8480
    },
    {
      "epoch": 0.9719839908519153,
      "grad_norm": 0.5485374927520752,
      "learning_rate": 0.00010281303602058318,
      "loss": 0.5122,
      "step": 8500
    },
    {
      "epoch": 0.9742710120068611,
      "grad_norm": 0.5305131673812866,
      "learning_rate": 0.00010258433390508864,
      "loss": 0.4771,
      "step": 8520
    },
    {
      "epoch": 0.9765580331618068,
      "grad_norm": 0.4918171465396881,
      "learning_rate": 0.00010235563178959406,
      "loss": 0.484,
      "step": 8540
    },
    {
      "epoch": 0.9788450543167524,
      "grad_norm": 0.7628704905509949,
      "learning_rate": 0.0001021269296740995,
      "loss": 0.5178,
      "step": 8560
    },
    {
      "epoch": 0.9811320754716981,
      "grad_norm": 0.7211087346076965,
      "learning_rate": 0.00010189822755860491,
      "loss": 0.5038,
      "step": 8580
    },
    {
      "epoch": 0.9834190966266438,
      "grad_norm": 0.6074907183647156,
      "learning_rate": 0.00010166952544311035,
      "loss": 0.4651,
      "step": 8600
    },
    {
      "epoch": 0.9857061177815895,
      "grad_norm": 0.5103387832641602,
      "learning_rate": 0.00010144082332761578,
      "loss": 0.4774,
      "step": 8620
    },
    {
      "epoch": 0.9879931389365352,
      "grad_norm": 0.5337165594100952,
      "learning_rate": 0.00010121212121212122,
      "loss": 0.524,
      "step": 8640
    },
    {
      "epoch": 0.9902801600914809,
      "grad_norm": 0.5146458148956299,
      "learning_rate": 0.00010098341909662666,
      "loss": 0.5076,
      "step": 8660
    },
    {
      "epoch": 0.9925671812464265,
      "grad_norm": 0.7087204456329346,
      "learning_rate": 0.00010075471698113208,
      "loss": 0.5036,
      "step": 8680
    },
    {
      "epoch": 0.9948542024013722,
      "grad_norm": 0.6216093897819519,
      "learning_rate": 0.00010052601486563751,
      "loss": 0.4836,
      "step": 8700
    },
    {
      "epoch": 0.9971412235563178,
      "grad_norm": 0.9503776431083679,
      "learning_rate": 0.00010029731275014294,
      "loss": 0.5033,
      "step": 8720
    },
    {
      "epoch": 0.9994282447112636,
      "grad_norm": 0.37365642189979553,
      "learning_rate": 0.00010006861063464839,
      "loss": 0.5298,
      "step": 8740
    },
    {
      "epoch": 1.0017152658662092,
      "grad_norm": 0.413610577583313,
      "learning_rate": 9.98399085191538e-05,
      "loss": 0.4942,
      "step": 8760
    },
    {
      "epoch": 1.004002287021155,
      "grad_norm": 0.6165034174919128,
      "learning_rate": 9.961120640365924e-05,
      "loss": 0.4761,
      "step": 8780
    },
    {
      "epoch": 1.0062893081761006,
      "grad_norm": 0.5336429476737976,
      "learning_rate": 9.938250428816467e-05,
      "loss": 0.449,
      "step": 8800
    },
    {
      "epoch": 1.0085763293310464,
      "grad_norm": 0.452433705329895,
      "learning_rate": 9.91538021726701e-05,
      "loss": 0.4607,
      "step": 8820
    },
    {
      "epoch": 1.010863350485992,
      "grad_norm": 0.7323898673057556,
      "learning_rate": 9.892510005717554e-05,
      "loss": 0.462,
      "step": 8840
    },
    {
      "epoch": 1.0131503716409376,
      "grad_norm": 0.543148398399353,
      "learning_rate": 9.869639794168097e-05,
      "loss": 0.5052,
      "step": 8860
    },
    {
      "epoch": 1.0154373927958833,
      "grad_norm": 0.49199414253234863,
      "learning_rate": 9.84676958261864e-05,
      "loss": 0.4527,
      "step": 8880
    },
    {
      "epoch": 1.017724413950829,
      "grad_norm": 0.5711661577224731,
      "learning_rate": 9.823899371069183e-05,
      "loss": 0.5085,
      "step": 8900
    },
    {
      "epoch": 1.0200114351057747,
      "grad_norm": 0.7828440070152283,
      "learning_rate": 9.801029159519725e-05,
      "loss": 0.4642,
      "step": 8920
    },
    {
      "epoch": 1.0222984562607205,
      "grad_norm": 0.8823119401931763,
      "learning_rate": 9.77815894797027e-05,
      "loss": 0.4769,
      "step": 8940
    },
    {
      "epoch": 1.0245854774156662,
      "grad_norm": 0.640278160572052,
      "learning_rate": 9.755288736420812e-05,
      "loss": 0.4343,
      "step": 8960
    },
    {
      "epoch": 1.0268724985706117,
      "grad_norm": 0.5962653756141663,
      "learning_rate": 9.732418524871355e-05,
      "loss": 0.5059,
      "step": 8980
    },
    {
      "epoch": 1.0291595197255574,
      "grad_norm": 0.5452612638473511,
      "learning_rate": 9.709548313321898e-05,
      "loss": 0.4668,
      "step": 9000
    },
    {
      "epoch": 1.0314465408805031,
      "grad_norm": 0.47367098927497864,
      "learning_rate": 9.686678101772442e-05,
      "loss": 0.4844,
      "step": 9020
    },
    {
      "epoch": 1.0337335620354489,
      "grad_norm": 0.5532112717628479,
      "learning_rate": 9.663807890222985e-05,
      "loss": 0.4872,
      "step": 9040
    },
    {
      "epoch": 1.0360205831903946,
      "grad_norm": 0.6877228021621704,
      "learning_rate": 9.640937678673528e-05,
      "loss": 0.4974,
      "step": 9060
    },
    {
      "epoch": 1.03830760434534,
      "grad_norm": 0.59063321352005,
      "learning_rate": 9.618067467124071e-05,
      "loss": 0.4499,
      "step": 9080
    },
    {
      "epoch": 1.0405946255002858,
      "grad_norm": 0.5624562501907349,
      "learning_rate": 9.595197255574615e-05,
      "loss": 0.4604,
      "step": 9100
    },
    {
      "epoch": 1.0428816466552315,
      "grad_norm": 0.6747223734855652,
      "learning_rate": 9.572327044025158e-05,
      "loss": 0.4745,
      "step": 9120
    },
    {
      "epoch": 1.0451686678101773,
      "grad_norm": 0.5706072449684143,
      "learning_rate": 9.549456832475701e-05,
      "loss": 0.4503,
      "step": 9140
    },
    {
      "epoch": 1.047455688965123,
      "grad_norm": 0.4161527752876282,
      "learning_rate": 9.526586620926244e-05,
      "loss": 0.4365,
      "step": 9160
    },
    {
      "epoch": 1.0497427101200687,
      "grad_norm": 0.5982766151428223,
      "learning_rate": 9.503716409376788e-05,
      "loss": 0.4662,
      "step": 9180
    },
    {
      "epoch": 1.0520297312750142,
      "grad_norm": 0.5014995336532593,
      "learning_rate": 9.48084619782733e-05,
      "loss": 0.4693,
      "step": 9200
    },
    {
      "epoch": 1.05431675242996,
      "grad_norm": 0.5946124196052551,
      "learning_rate": 9.457975986277874e-05,
      "loss": 0.4955,
      "step": 9220
    },
    {
      "epoch": 1.0566037735849056,
      "grad_norm": 0.587039053440094,
      "learning_rate": 9.435105774728416e-05,
      "loss": 0.4815,
      "step": 9240
    },
    {
      "epoch": 1.0588907947398514,
      "grad_norm": 0.6369040012359619,
      "learning_rate": 9.412235563178959e-05,
      "loss": 0.4639,
      "step": 9260
    },
    {
      "epoch": 1.061177815894797,
      "grad_norm": 1.0581923723220825,
      "learning_rate": 9.389365351629504e-05,
      "loss": 0.4799,
      "step": 9280
    },
    {
      "epoch": 1.0634648370497426,
      "grad_norm": 0.5603721141815186,
      "learning_rate": 9.366495140080046e-05,
      "loss": 0.4687,
      "step": 9300
    },
    {
      "epoch": 1.0657518582046883,
      "grad_norm": 0.5610836148262024,
      "learning_rate": 9.34362492853059e-05,
      "loss": 0.4867,
      "step": 9320
    },
    {
      "epoch": 1.068038879359634,
      "grad_norm": 0.6039822101593018,
      "learning_rate": 9.320754716981132e-05,
      "loss": 0.5051,
      "step": 9340
    },
    {
      "epoch": 1.0703259005145798,
      "grad_norm": 0.5503366589546204,
      "learning_rate": 9.297884505431675e-05,
      "loss": 0.4667,
      "step": 9360
    },
    {
      "epoch": 1.0726129216695255,
      "grad_norm": 0.7015678286552429,
      "learning_rate": 9.275014293882219e-05,
      "loss": 0.4796,
      "step": 9380
    },
    {
      "epoch": 1.0748999428244712,
      "grad_norm": 0.6514174938201904,
      "learning_rate": 9.252144082332762e-05,
      "loss": 0.5185,
      "step": 9400
    },
    {
      "epoch": 1.0771869639794167,
      "grad_norm": 0.7096505761146545,
      "learning_rate": 9.229273870783305e-05,
      "loss": 0.4813,
      "step": 9420
    },
    {
      "epoch": 1.0794739851343624,
      "grad_norm": 0.5854485630989075,
      "learning_rate": 9.206403659233848e-05,
      "loss": 0.4891,
      "step": 9440
    },
    {
      "epoch": 1.0817610062893082,
      "grad_norm": 0.706356942653656,
      "learning_rate": 9.183533447684392e-05,
      "loss": 0.4873,
      "step": 9460
    },
    {
      "epoch": 1.0840480274442539,
      "grad_norm": 0.4793562889099121,
      "learning_rate": 9.160663236134934e-05,
      "loss": 0.4853,
      "step": 9480
    },
    {
      "epoch": 1.0863350485991996,
      "grad_norm": 0.6417852640151978,
      "learning_rate": 9.137793024585478e-05,
      "loss": 0.4552,
      "step": 9500
    },
    {
      "epoch": 1.0886220697541453,
      "grad_norm": 0.7854524254798889,
      "learning_rate": 9.114922813036022e-05,
      "loss": 0.4945,
      "step": 9520
    },
    {
      "epoch": 1.0909090909090908,
      "grad_norm": 0.7707622647285461,
      "learning_rate": 9.092052601486563e-05,
      "loss": 0.4535,
      "step": 9540
    },
    {
      "epoch": 1.0931961120640366,
      "grad_norm": 0.6969077587127686,
      "learning_rate": 9.069182389937108e-05,
      "loss": 0.4664,
      "step": 9560
    },
    {
      "epoch": 1.0954831332189823,
      "grad_norm": 0.7229498028755188,
      "learning_rate": 9.04631217838765e-05,
      "loss": 0.4506,
      "step": 9580
    },
    {
      "epoch": 1.097770154373928,
      "grad_norm": 0.6389006972312927,
      "learning_rate": 9.023441966838195e-05,
      "loss": 0.4949,
      "step": 9600
    },
    {
      "epoch": 1.1000571755288737,
      "grad_norm": 0.5304635763168335,
      "learning_rate": 9.000571755288736e-05,
      "loss": 0.4754,
      "step": 9620
    },
    {
      "epoch": 1.1023441966838192,
      "grad_norm": 0.6120285987854004,
      "learning_rate": 8.97770154373928e-05,
      "loss": 0.4385,
      "step": 9640
    },
    {
      "epoch": 1.104631217838765,
      "grad_norm": 0.5339592099189758,
      "learning_rate": 8.954831332189824e-05,
      "loss": 0.4493,
      "step": 9660
    },
    {
      "epoch": 1.1069182389937107,
      "grad_norm": 0.5261082053184509,
      "learning_rate": 8.931961120640366e-05,
      "loss": 0.4445,
      "step": 9680
    },
    {
      "epoch": 1.1092052601486564,
      "grad_norm": 0.810175895690918,
      "learning_rate": 8.90909090909091e-05,
      "loss": 0.4765,
      "step": 9700
    },
    {
      "epoch": 1.1114922813036021,
      "grad_norm": 0.6198227405548096,
      "learning_rate": 8.886220697541453e-05,
      "loss": 0.4462,
      "step": 9720
    },
    {
      "epoch": 1.1137793024585478,
      "grad_norm": 0.5542476773262024,
      "learning_rate": 8.863350485991996e-05,
      "loss": 0.4495,
      "step": 9740
    },
    {
      "epoch": 1.1160663236134933,
      "grad_norm": 0.5690268874168396,
      "learning_rate": 8.840480274442539e-05,
      "loss": 0.4781,
      "step": 9760
    },
    {
      "epoch": 1.118353344768439,
      "grad_norm": 0.5042832493782043,
      "learning_rate": 8.817610062893082e-05,
      "loss": 0.4988,
      "step": 9780
    },
    {
      "epoch": 1.1206403659233848,
      "grad_norm": 0.5570304989814758,
      "learning_rate": 8.794739851343626e-05,
      "loss": 0.4643,
      "step": 9800
    },
    {
      "epoch": 1.1229273870783305,
      "grad_norm": 0.4985276162624359,
      "learning_rate": 8.771869639794168e-05,
      "loss": 0.4099,
      "step": 9820
    },
    {
      "epoch": 1.1252144082332762,
      "grad_norm": 0.7109980583190918,
      "learning_rate": 8.748999428244712e-05,
      "loss": 0.4377,
      "step": 9840
    },
    {
      "epoch": 1.127501429388222,
      "grad_norm": 0.5686187744140625,
      "learning_rate": 8.726129216695254e-05,
      "loss": 0.4736,
      "step": 9860
    },
    {
      "epoch": 1.1297884505431675,
      "grad_norm": 0.5459140539169312,
      "learning_rate": 8.703259005145799e-05,
      "loss": 0.4672,
      "step": 9880
    },
    {
      "epoch": 1.1320754716981132,
      "grad_norm": 0.7450047731399536,
      "learning_rate": 8.680388793596342e-05,
      "loss": 0.488,
      "step": 9900
    },
    {
      "epoch": 1.134362492853059,
      "grad_norm": 0.6232065558433533,
      "learning_rate": 8.657518582046884e-05,
      "loss": 0.4935,
      "step": 9920
    },
    {
      "epoch": 1.1366495140080046,
      "grad_norm": 0.5953841805458069,
      "learning_rate": 8.634648370497428e-05,
      "loss": 0.4887,
      "step": 9940
    },
    {
      "epoch": 1.1389365351629503,
      "grad_norm": 0.7553718090057373,
      "learning_rate": 8.61177815894797e-05,
      "loss": 0.4404,
      "step": 9960
    },
    {
      "epoch": 1.1412235563178958,
      "grad_norm": 0.5381597876548767,
      "learning_rate": 8.588907947398514e-05,
      "loss": 0.4806,
      "step": 9980
    },
    {
      "epoch": 1.1435105774728416,
      "grad_norm": 0.6029928922653198,
      "learning_rate": 8.566037735849057e-05,
      "loss": 0.4806,
      "step": 10000
    },
    {
      "epoch": 1.1457975986277873,
      "grad_norm": 0.965060293674469,
      "learning_rate": 8.5431675242996e-05,
      "loss": 0.5015,
      "step": 10020
    },
    {
      "epoch": 1.148084619782733,
      "grad_norm": 0.5703927874565125,
      "learning_rate": 8.520297312750143e-05,
      "loss": 0.4777,
      "step": 10040
    },
    {
      "epoch": 1.1503716409376787,
      "grad_norm": 0.6055569648742676,
      "learning_rate": 8.497427101200687e-05,
      "loss": 0.4411,
      "step": 10060
    },
    {
      "epoch": 1.1526586620926245,
      "grad_norm": 0.6485864520072937,
      "learning_rate": 8.47455688965123e-05,
      "loss": 0.4666,
      "step": 10080
    },
    {
      "epoch": 1.15494568324757,
      "grad_norm": 0.4030531644821167,
      "learning_rate": 8.451686678101773e-05,
      "loss": 0.4109,
      "step": 10100
    },
    {
      "epoch": 1.1572327044025157,
      "grad_norm": 0.6332659125328064,
      "learning_rate": 8.428816466552316e-05,
      "loss": 0.4708,
      "step": 10120
    },
    {
      "epoch": 1.1595197255574614,
      "grad_norm": 0.5699838399887085,
      "learning_rate": 8.405946255002858e-05,
      "loss": 0.4792,
      "step": 10140
    },
    {
      "epoch": 1.1618067467124071,
      "grad_norm": 0.7469475269317627,
      "learning_rate": 8.383076043453403e-05,
      "loss": 0.4344,
      "step": 10160
    },
    {
      "epoch": 1.1640937678673529,
      "grad_norm": 0.8195727467536926,
      "learning_rate": 8.360205831903946e-05,
      "loss": 0.5247,
      "step": 10180
    },
    {
      "epoch": 1.1663807890222984,
      "grad_norm": 0.466838538646698,
      "learning_rate": 8.337335620354488e-05,
      "loss": 0.4697,
      "step": 10200
    },
    {
      "epoch": 1.168667810177244,
      "grad_norm": 0.69560307264328,
      "learning_rate": 8.314465408805033e-05,
      "loss": 0.4465,
      "step": 10220
    },
    {
      "epoch": 1.1709548313321898,
      "grad_norm": 0.7467725872993469,
      "learning_rate": 8.291595197255575e-05,
      "loss": 0.416,
      "step": 10240
    },
    {
      "epoch": 1.1732418524871355,
      "grad_norm": 0.6806586980819702,
      "learning_rate": 8.268724985706118e-05,
      "loss": 0.4482,
      "step": 10260
    },
    {
      "epoch": 1.1755288736420813,
      "grad_norm": 0.5327321290969849,
      "learning_rate": 8.245854774156661e-05,
      "loss": 0.4557,
      "step": 10280
    },
    {
      "epoch": 1.177815894797027,
      "grad_norm": 0.663781464099884,
      "learning_rate": 8.222984562607204e-05,
      "loss": 0.4927,
      "step": 10300
    },
    {
      "epoch": 1.1801029159519725,
      "grad_norm": 0.7606556415557861,
      "learning_rate": 8.200114351057748e-05,
      "loss": 0.4952,
      "step": 10320
    },
    {
      "epoch": 1.1823899371069182,
      "grad_norm": 0.6988136768341064,
      "learning_rate": 8.177244139508291e-05,
      "loss": 0.4498,
      "step": 10340
    },
    {
      "epoch": 1.184676958261864,
      "grad_norm": 0.8272442817687988,
      "learning_rate": 8.154373927958834e-05,
      "loss": 0.4701,
      "step": 10360
    },
    {
      "epoch": 1.1869639794168096,
      "grad_norm": 0.5726200938224792,
      "learning_rate": 8.131503716409377e-05,
      "loss": 0.4445,
      "step": 10380
    },
    {
      "epoch": 1.1892510005717554,
      "grad_norm": 0.6617915034294128,
      "learning_rate": 8.10863350485992e-05,
      "loss": 0.4836,
      "step": 10400
    },
    {
      "epoch": 1.1915380217267009,
      "grad_norm": 0.7213875651359558,
      "learning_rate": 8.085763293310464e-05,
      "loss": 0.4787,
      "step": 10420
    },
    {
      "epoch": 1.1938250428816466,
      "grad_norm": 0.573653519153595,
      "learning_rate": 8.062893081761007e-05,
      "loss": 0.5144,
      "step": 10440
    },
    {
      "epoch": 1.1961120640365923,
      "grad_norm": 0.748302698135376,
      "learning_rate": 8.04002287021155e-05,
      "loss": 0.5021,
      "step": 10460
    },
    {
      "epoch": 1.198399085191538,
      "grad_norm": 0.6876729726791382,
      "learning_rate": 8.017152658662092e-05,
      "loss": 0.4988,
      "step": 10480
    },
    {
      "epoch": 1.2006861063464838,
      "grad_norm": 0.6129347681999207,
      "learning_rate": 7.994282447112637e-05,
      "loss": 0.5175,
      "step": 10500
    },
    {
      "epoch": 1.2029731275014295,
      "grad_norm": 0.6966704726219177,
      "learning_rate": 7.971412235563179e-05,
      "loss": 0.4707,
      "step": 10520
    },
    {
      "epoch": 1.205260148656375,
      "grad_norm": 0.6155797839164734,
      "learning_rate": 7.948542024013722e-05,
      "loss": 0.4557,
      "step": 10540
    },
    {
      "epoch": 1.2075471698113207,
      "grad_norm": 0.7061171531677246,
      "learning_rate": 7.925671812464267e-05,
      "loss": 0.4804,
      "step": 10560
    },
    {
      "epoch": 1.2098341909662664,
      "grad_norm": 0.6329413056373596,
      "learning_rate": 7.902801600914808e-05,
      "loss": 0.4828,
      "step": 10580
    },
    {
      "epoch": 1.2121212121212122,
      "grad_norm": 0.5729008316993713,
      "learning_rate": 7.879931389365352e-05,
      "loss": 0.4639,
      "step": 10600
    },
    {
      "epoch": 1.2144082332761579,
      "grad_norm": 0.6591130495071411,
      "learning_rate": 7.857061177815895e-05,
      "loss": 0.4662,
      "step": 10620
    },
    {
      "epoch": 1.2166952544311034,
      "grad_norm": 0.7141635417938232,
      "learning_rate": 7.834190966266438e-05,
      "loss": 0.5013,
      "step": 10640
    },
    {
      "epoch": 1.218982275586049,
      "grad_norm": 0.41972219944000244,
      "learning_rate": 7.811320754716981e-05,
      "loss": 0.4807,
      "step": 10660
    },
    {
      "epoch": 1.2212692967409948,
      "grad_norm": 0.7906423211097717,
      "learning_rate": 7.788450543167525e-05,
      "loss": 0.4834,
      "step": 10680
    },
    {
      "epoch": 1.2235563178959405,
      "grad_norm": 0.7250064611434937,
      "learning_rate": 7.765580331618068e-05,
      "loss": 0.4658,
      "step": 10700
    },
    {
      "epoch": 1.2258433390508863,
      "grad_norm": 0.5177663564682007,
      "learning_rate": 7.742710120068611e-05,
      "loss": 0.4686,
      "step": 10720
    },
    {
      "epoch": 1.228130360205832,
      "grad_norm": 0.8611894845962524,
      "learning_rate": 7.719839908519154e-05,
      "loss": 0.4618,
      "step": 10740
    },
    {
      "epoch": 1.2304173813607775,
      "grad_norm": 0.5994923114776611,
      "learning_rate": 7.696969696969696e-05,
      "loss": 0.4632,
      "step": 10760
    },
    {
      "epoch": 1.2327044025157232,
      "grad_norm": 0.8992072939872742,
      "learning_rate": 7.674099485420241e-05,
      "loss": 0.4802,
      "step": 10780
    },
    {
      "epoch": 1.234991423670669,
      "grad_norm": 0.5909512639045715,
      "learning_rate": 7.651229273870784e-05,
      "loss": 0.4721,
      "step": 10800
    },
    {
      "epoch": 1.2372784448256147,
      "grad_norm": 0.6940491795539856,
      "learning_rate": 7.628359062321326e-05,
      "loss": 0.4034,
      "step": 10820
    },
    {
      "epoch": 1.2395654659805604,
      "grad_norm": 0.5677328705787659,
      "learning_rate": 7.605488850771871e-05,
      "loss": 0.4497,
      "step": 10840
    },
    {
      "epoch": 1.241852487135506,
      "grad_norm": 0.5245732665061951,
      "learning_rate": 7.582618639222413e-05,
      "loss": 0.4221,
      "step": 10860
    },
    {
      "epoch": 1.2441395082904516,
      "grad_norm": 0.7014197707176208,
      "learning_rate": 7.559748427672957e-05,
      "loss": 0.4444,
      "step": 10880
    },
    {
      "epoch": 1.2464265294453973,
      "grad_norm": 0.591334879398346,
      "learning_rate": 7.536878216123499e-05,
      "loss": 0.4984,
      "step": 10900
    },
    {
      "epoch": 1.248713550600343,
      "grad_norm": 0.555702269077301,
      "learning_rate": 7.514008004574042e-05,
      "loss": 0.4406,
      "step": 10920
    },
    {
      "epoch": 1.2510005717552888,
      "grad_norm": 0.7097614407539368,
      "learning_rate": 7.491137793024586e-05,
      "loss": 0.4562,
      "step": 10940
    },
    {
      "epoch": 1.2532875929102345,
      "grad_norm": 0.7031176090240479,
      "learning_rate": 7.468267581475129e-05,
      "loss": 0.5286,
      "step": 10960
    },
    {
      "epoch": 1.2555746140651802,
      "grad_norm": 0.6150069236755371,
      "learning_rate": 7.445397369925672e-05,
      "loss": 0.4653,
      "step": 10980
    },
    {
      "epoch": 1.2578616352201257,
      "grad_norm": 0.564446210861206,
      "learning_rate": 7.422527158376215e-05,
      "loss": 0.4286,
      "step": 11000
    },
    {
      "epoch": 1.2601486563750715,
      "grad_norm": 0.6820585131645203,
      "learning_rate": 7.399656946826759e-05,
      "loss": 0.4622,
      "step": 11020
    },
    {
      "epoch": 1.2624356775300172,
      "grad_norm": 0.6254857182502747,
      "learning_rate": 7.376786735277302e-05,
      "loss": 0.4714,
      "step": 11040
    },
    {
      "epoch": 1.264722698684963,
      "grad_norm": 0.6361508965492249,
      "learning_rate": 7.353916523727845e-05,
      "loss": 0.4761,
      "step": 11060
    },
    {
      "epoch": 1.2670097198399084,
      "grad_norm": 0.7183345556259155,
      "learning_rate": 7.331046312178388e-05,
      "loss": 0.4979,
      "step": 11080
    },
    {
      "epoch": 1.2692967409948541,
      "grad_norm": 0.7000192403793335,
      "learning_rate": 7.30817610062893e-05,
      "loss": 0.4401,
      "step": 11100
    },
    {
      "epoch": 1.2715837621497998,
      "grad_norm": 0.7662758827209473,
      "learning_rate": 7.285305889079475e-05,
      "loss": 0.484,
      "step": 11120
    },
    {
      "epoch": 1.2738707833047456,
      "grad_norm": 0.4796141982078552,
      "learning_rate": 7.262435677530017e-05,
      "loss": 0.4429,
      "step": 11140
    },
    {
      "epoch": 1.2761578044596913,
      "grad_norm": 0.5409734845161438,
      "learning_rate": 7.239565465980561e-05,
      "loss": 0.483,
      "step": 11160
    },
    {
      "epoch": 1.278444825614637,
      "grad_norm": 0.7179098725318909,
      "learning_rate": 7.216695254431103e-05,
      "loss": 0.4676,
      "step": 11180
    },
    {
      "epoch": 1.2807318467695827,
      "grad_norm": 0.5823912620544434,
      "learning_rate": 7.193825042881647e-05,
      "loss": 0.4096,
      "step": 11200
    },
    {
      "epoch": 1.2830188679245282,
      "grad_norm": 0.6904700398445129,
      "learning_rate": 7.170954831332191e-05,
      "loss": 0.4705,
      "step": 11220
    },
    {
      "epoch": 1.285305889079474,
      "grad_norm": 0.6046538949012756,
      "learning_rate": 7.148084619782733e-05,
      "loss": 0.4771,
      "step": 11240
    },
    {
      "epoch": 1.2875929102344197,
      "grad_norm": 0.7286409139633179,
      "learning_rate": 7.125214408233276e-05,
      "loss": 0.4627,
      "step": 11260
    },
    {
      "epoch": 1.2898799313893654,
      "grad_norm": 0.7727022171020508,
      "learning_rate": 7.10234419668382e-05,
      "loss": 0.4807,
      "step": 11280
    },
    {
      "epoch": 1.292166952544311,
      "grad_norm": 0.8039162158966064,
      "learning_rate": 7.079473985134363e-05,
      "loss": 0.4675,
      "step": 11300
    },
    {
      "epoch": 1.2944539736992566,
      "grad_norm": 0.5716332793235779,
      "learning_rate": 7.056603773584906e-05,
      "loss": 0.4461,
      "step": 11320
    },
    {
      "epoch": 1.2967409948542024,
      "grad_norm": 0.6909472346305847,
      "learning_rate": 7.03373356203545e-05,
      "loss": 0.4799,
      "step": 11340
    },
    {
      "epoch": 1.299028016009148,
      "grad_norm": 0.6108283996582031,
      "learning_rate": 7.010863350485993e-05,
      "loss": 0.4074,
      "step": 11360
    },
    {
      "epoch": 1.3013150371640938,
      "grad_norm": 0.5046330094337463,
      "learning_rate": 6.987993138936534e-05,
      "loss": 0.4956,
      "step": 11380
    },
    {
      "epoch": 1.3036020583190395,
      "grad_norm": 0.6515011191368103,
      "learning_rate": 6.965122927387079e-05,
      "loss": 0.4612,
      "step": 11400
    },
    {
      "epoch": 1.3058890794739852,
      "grad_norm": 0.5307015776634216,
      "learning_rate": 6.942252715837621e-05,
      "loss": 0.4606,
      "step": 11420
    },
    {
      "epoch": 1.3081761006289307,
      "grad_norm": 0.6423569321632385,
      "learning_rate": 6.919382504288166e-05,
      "loss": 0.5192,
      "step": 11440
    },
    {
      "epoch": 1.3104631217838765,
      "grad_norm": 0.6485777497291565,
      "learning_rate": 6.896512292738709e-05,
      "loss": 0.4963,
      "step": 11460
    },
    {
      "epoch": 1.3127501429388222,
      "grad_norm": 0.5746749639511108,
      "learning_rate": 6.873642081189251e-05,
      "loss": 0.4516,
      "step": 11480
    },
    {
      "epoch": 1.315037164093768,
      "grad_norm": 0.6877344846725464,
      "learning_rate": 6.850771869639795e-05,
      "loss": 0.4713,
      "step": 11500
    },
    {
      "epoch": 1.3173241852487134,
      "grad_norm": 0.7198066115379333,
      "learning_rate": 6.827901658090337e-05,
      "loss": 0.4917,
      "step": 11520
    },
    {
      "epoch": 1.3196112064036591,
      "grad_norm": 0.8176125288009644,
      "learning_rate": 6.80503144654088e-05,
      "loss": 0.4686,
      "step": 11540
    },
    {
      "epoch": 1.3218982275586049,
      "grad_norm": 0.6019795536994934,
      "learning_rate": 6.782161234991424e-05,
      "loss": 0.4622,
      "step": 11560
    },
    {
      "epoch": 1.3241852487135506,
      "grad_norm": 0.5696286559104919,
      "learning_rate": 6.759291023441967e-05,
      "loss": 0.4672,
      "step": 11580
    },
    {
      "epoch": 1.3264722698684963,
      "grad_norm": 0.985514223575592,
      "learning_rate": 6.73642081189251e-05,
      "loss": 0.5001,
      "step": 11600
    },
    {
      "epoch": 1.328759291023442,
      "grad_norm": 0.5892218947410583,
      "learning_rate": 6.713550600343054e-05,
      "loss": 0.4664,
      "step": 11620
    },
    {
      "epoch": 1.3310463121783878,
      "grad_norm": 0.5219250917434692,
      "learning_rate": 6.690680388793597e-05,
      "loss": 0.4496,
      "step": 11640
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.7592542171478271,
      "learning_rate": 6.66781017724414e-05,
      "loss": 0.4748,
      "step": 11660
    },
    {
      "epoch": 1.335620354488279,
      "grad_norm": 0.7074798345565796,
      "learning_rate": 6.644939965694683e-05,
      "loss": 0.5141,
      "step": 11680
    },
    {
      "epoch": 1.3379073756432247,
      "grad_norm": 0.6644710898399353,
      "learning_rate": 6.622069754145227e-05,
      "loss": 0.4565,
      "step": 11700
    },
    {
      "epoch": 1.3401943967981704,
      "grad_norm": 0.632127046585083,
      "learning_rate": 6.59919954259577e-05,
      "loss": 0.4478,
      "step": 11720
    },
    {
      "epoch": 1.342481417953116,
      "grad_norm": 0.6138166785240173,
      "learning_rate": 6.576329331046313e-05,
      "loss": 0.5311,
      "step": 11740
    },
    {
      "epoch": 1.3447684391080617,
      "grad_norm": 0.7210965156555176,
      "learning_rate": 6.553459119496855e-05,
      "loss": 0.4682,
      "step": 11760
    },
    {
      "epoch": 1.3470554602630074,
      "grad_norm": 0.6738827228546143,
      "learning_rate": 6.5305889079474e-05,
      "loss": 0.432,
      "step": 11780
    },
    {
      "epoch": 1.349342481417953,
      "grad_norm": 1.0584396123886108,
      "learning_rate": 6.507718696397941e-05,
      "loss": 0.4897,
      "step": 11800
    },
    {
      "epoch": 1.3516295025728988,
      "grad_norm": 1.1195985078811646,
      "learning_rate": 6.484848484848485e-05,
      "loss": 0.4647,
      "step": 11820
    },
    {
      "epoch": 1.3539165237278445,
      "grad_norm": 0.709047794342041,
      "learning_rate": 6.461978273299028e-05,
      "loss": 0.4754,
      "step": 11840
    },
    {
      "epoch": 1.3562035448827903,
      "grad_norm": 0.7751067280769348,
      "learning_rate": 6.439108061749571e-05,
      "loss": 0.4611,
      "step": 11860
    },
    {
      "epoch": 1.3584905660377358,
      "grad_norm": 0.4331466853618622,
      "learning_rate": 6.416237850200114e-05,
      "loss": 0.4997,
      "step": 11880
    },
    {
      "epoch": 1.3607775871926815,
      "grad_norm": 0.49089258909225464,
      "learning_rate": 6.393367638650658e-05,
      "loss": 0.5026,
      "step": 11900
    },
    {
      "epoch": 1.3630646083476272,
      "grad_norm": 0.6986894011497498,
      "learning_rate": 6.370497427101201e-05,
      "loss": 0.4808,
      "step": 11920
    },
    {
      "epoch": 1.365351629502573,
      "grad_norm": 0.946709156036377,
      "learning_rate": 6.347627215551744e-05,
      "loss": 0.4339,
      "step": 11940
    },
    {
      "epoch": 1.3676386506575187,
      "grad_norm": 0.517097532749176,
      "learning_rate": 6.324757004002287e-05,
      "loss": 0.4882,
      "step": 11960
    },
    {
      "epoch": 1.3699256718124642,
      "grad_norm": 0.6099753379821777,
      "learning_rate": 6.301886792452831e-05,
      "loss": 0.4661,
      "step": 11980
    },
    {
      "epoch": 1.3722126929674099,
      "grad_norm": 0.9663565754890442,
      "learning_rate": 6.279016580903374e-05,
      "loss": 0.45,
      "step": 12000
    },
    {
      "epoch": 1.3744997141223556,
      "grad_norm": 0.6706234812736511,
      "learning_rate": 6.256146369353917e-05,
      "loss": 0.4485,
      "step": 12020
    },
    {
      "epoch": 1.3767867352773013,
      "grad_norm": 0.47755125164985657,
      "learning_rate": 6.233276157804459e-05,
      "loss": 0.4876,
      "step": 12040
    },
    {
      "epoch": 1.379073756432247,
      "grad_norm": 0.6561873555183411,
      "learning_rate": 6.210405946255004e-05,
      "loss": 0.4337,
      "step": 12060
    },
    {
      "epoch": 1.3813607775871928,
      "grad_norm": 0.7669251561164856,
      "learning_rate": 6.187535734705546e-05,
      "loss": 0.4894,
      "step": 12080
    },
    {
      "epoch": 1.3836477987421385,
      "grad_norm": 0.8337940573692322,
      "learning_rate": 6.164665523156089e-05,
      "loss": 0.4423,
      "step": 12100
    },
    {
      "epoch": 1.385934819897084,
      "grad_norm": 0.6930534243583679,
      "learning_rate": 6.141795311606633e-05,
      "loss": 0.4866,
      "step": 12120
    },
    {
      "epoch": 1.3882218410520297,
      "grad_norm": 1.0435552597045898,
      "learning_rate": 6.118925100057175e-05,
      "loss": 0.4311,
      "step": 12140
    },
    {
      "epoch": 1.3905088622069754,
      "grad_norm": 0.8225405812263489,
      "learning_rate": 6.0960548885077186e-05,
      "loss": 0.4739,
      "step": 12160
    },
    {
      "epoch": 1.3927958833619212,
      "grad_norm": 0.6142464876174927,
      "learning_rate": 6.0731846769582626e-05,
      "loss": 0.4504,
      "step": 12180
    },
    {
      "epoch": 1.3950829045168667,
      "grad_norm": 0.6073923110961914,
      "learning_rate": 6.050314465408805e-05,
      "loss": 0.5448,
      "step": 12200
    },
    {
      "epoch": 1.3973699256718124,
      "grad_norm": 0.6960263848304749,
      "learning_rate": 6.027444253859349e-05,
      "loss": 0.4825,
      "step": 12220
    },
    {
      "epoch": 1.3996569468267581,
      "grad_norm": 0.7163681983947754,
      "learning_rate": 6.0045740423098916e-05,
      "loss": 0.4991,
      "step": 12240
    },
    {
      "epoch": 1.4019439679817038,
      "grad_norm": 0.5719480514526367,
      "learning_rate": 5.981703830760434e-05,
      "loss": 0.44,
      "step": 12260
    },
    {
      "epoch": 1.4042309891366496,
      "grad_norm": 0.49364224076271057,
      "learning_rate": 5.958833619210978e-05,
      "loss": 0.4712,
      "step": 12280
    },
    {
      "epoch": 1.4065180102915953,
      "grad_norm": 0.9026888012886047,
      "learning_rate": 5.9359634076615214e-05,
      "loss": 0.4277,
      "step": 12300
    },
    {
      "epoch": 1.408805031446541,
      "grad_norm": 0.579943060874939,
      "learning_rate": 5.913093196112064e-05,
      "loss": 0.4902,
      "step": 12320
    },
    {
      "epoch": 1.4110920526014865,
      "grad_norm": 0.821577787399292,
      "learning_rate": 5.890222984562608e-05,
      "loss": 0.4563,
      "step": 12340
    },
    {
      "epoch": 1.4133790737564322,
      "grad_norm": 0.7380978465080261,
      "learning_rate": 5.8673527730131505e-05,
      "loss": 0.4632,
      "step": 12360
    },
    {
      "epoch": 1.415666094911378,
      "grad_norm": 0.5862332582473755,
      "learning_rate": 5.844482561463693e-05,
      "loss": 0.4588,
      "step": 12380
    },
    {
      "epoch": 1.4179531160663237,
      "grad_norm": 0.7218972444534302,
      "learning_rate": 5.821612349914237e-05,
      "loss": 0.5002,
      "step": 12400
    },
    {
      "epoch": 1.4202401372212692,
      "grad_norm": 0.41421476006507874,
      "learning_rate": 5.79874213836478e-05,
      "loss": 0.4033,
      "step": 12420
    },
    {
      "epoch": 1.422527158376215,
      "grad_norm": 0.4723556935787201,
      "learning_rate": 5.775871926815324e-05,
      "loss": 0.4772,
      "step": 12440
    },
    {
      "epoch": 1.4248141795311606,
      "grad_norm": 0.7536136507987976,
      "learning_rate": 5.753001715265867e-05,
      "loss": 0.5179,
      "step": 12460
    },
    {
      "epoch": 1.4271012006861064,
      "grad_norm": 0.681566596031189,
      "learning_rate": 5.730131503716409e-05,
      "loss": 0.4646,
      "step": 12480
    },
    {
      "epoch": 1.429388221841052,
      "grad_norm": 0.4797409772872925,
      "learning_rate": 5.707261292166953e-05,
      "loss": 0.5146,
      "step": 12500
    },
    {
      "epoch": 1.4316752429959978,
      "grad_norm": 0.6866660118103027,
      "learning_rate": 5.684391080617496e-05,
      "loss": 0.4524,
      "step": 12520
    },
    {
      "epoch": 1.4339622641509435,
      "grad_norm": 0.6727272868156433,
      "learning_rate": 5.661520869068039e-05,
      "loss": 0.4921,
      "step": 12540
    },
    {
      "epoch": 1.436249285305889,
      "grad_norm": 0.6253437995910645,
      "learning_rate": 5.638650657518583e-05,
      "loss": 0.5155,
      "step": 12560
    },
    {
      "epoch": 1.4385363064608347,
      "grad_norm": 0.7406617403030396,
      "learning_rate": 5.6157804459691256e-05,
      "loss": 0.4511,
      "step": 12580
    },
    {
      "epoch": 1.4408233276157805,
      "grad_norm": 0.543317437171936,
      "learning_rate": 5.592910234419668e-05,
      "loss": 0.4973,
      "step": 12600
    },
    {
      "epoch": 1.4431103487707262,
      "grad_norm": 0.5698521733283997,
      "learning_rate": 5.570040022870212e-05,
      "loss": 0.48,
      "step": 12620
    },
    {
      "epoch": 1.4453973699256717,
      "grad_norm": 0.5402041077613831,
      "learning_rate": 5.5471698113207547e-05,
      "loss": 0.4252,
      "step": 12640
    },
    {
      "epoch": 1.4476843910806174,
      "grad_norm": 0.5713329911231995,
      "learning_rate": 5.524299599771298e-05,
      "loss": 0.4784,
      "step": 12660
    },
    {
      "epoch": 1.4499714122355631,
      "grad_norm": 0.5647489428520203,
      "learning_rate": 5.501429388221842e-05,
      "loss": 0.4809,
      "step": 12680
    },
    {
      "epoch": 1.4522584333905089,
      "grad_norm": 0.809837281703949,
      "learning_rate": 5.4785591766723844e-05,
      "loss": 0.46,
      "step": 12700
    },
    {
      "epoch": 1.4545454545454546,
      "grad_norm": 0.7051699161529541,
      "learning_rate": 5.4556889651229283e-05,
      "loss": 0.4547,
      "step": 12720
    },
    {
      "epoch": 1.4568324757004003,
      "grad_norm": 0.6884256601333618,
      "learning_rate": 5.432818753573471e-05,
      "loss": 0.4893,
      "step": 12740
    },
    {
      "epoch": 1.459119496855346,
      "grad_norm": 0.6134693026542664,
      "learning_rate": 5.4099485420240135e-05,
      "loss": 0.428,
      "step": 12760
    },
    {
      "epoch": 1.4614065180102915,
      "grad_norm": 0.8248193264007568,
      "learning_rate": 5.3870783304745574e-05,
      "loss": 0.4593,
      "step": 12780
    },
    {
      "epoch": 1.4636935391652373,
      "grad_norm": 0.799176812171936,
      "learning_rate": 5.364208118925101e-05,
      "loss": 0.4555,
      "step": 12800
    },
    {
      "epoch": 1.465980560320183,
      "grad_norm": 1.0510433912277222,
      "learning_rate": 5.341337907375643e-05,
      "loss": 0.5016,
      "step": 12820
    },
    {
      "epoch": 1.4682675814751287,
      "grad_norm": 0.5394272804260254,
      "learning_rate": 5.318467695826187e-05,
      "loss": 0.4429,
      "step": 12840
    },
    {
      "epoch": 1.4705546026300742,
      "grad_norm": 0.9046295881271362,
      "learning_rate": 5.29559748427673e-05,
      "loss": 0.4394,
      "step": 12860
    },
    {
      "epoch": 1.47284162378502,
      "grad_norm": 0.6007372140884399,
      "learning_rate": 5.272727272727272e-05,
      "loss": 0.4613,
      "step": 12880
    },
    {
      "epoch": 1.4751286449399656,
      "grad_norm": 0.6160247921943665,
      "learning_rate": 5.249857061177816e-05,
      "loss": 0.4808,
      "step": 12900
    },
    {
      "epoch": 1.4774156660949114,
      "grad_norm": 0.7786606550216675,
      "learning_rate": 5.226986849628359e-05,
      "loss": 0.4558,
      "step": 12920
    },
    {
      "epoch": 1.479702687249857,
      "grad_norm": 0.8044310212135315,
      "learning_rate": 5.204116638078902e-05,
      "loss": 0.467,
      "step": 12940
    },
    {
      "epoch": 1.4819897084048028,
      "grad_norm": 0.9881356954574585,
      "learning_rate": 5.181246426529446e-05,
      "loss": 0.4532,
      "step": 12960
    },
    {
      "epoch": 1.4842767295597485,
      "grad_norm": 0.6814484596252441,
      "learning_rate": 5.1583762149799886e-05,
      "loss": 0.4971,
      "step": 12980
    },
    {
      "epoch": 1.486563750714694,
      "grad_norm": 0.45566055178642273,
      "learning_rate": 5.1355060034305325e-05,
      "loss": 0.4476,
      "step": 13000
    },
    {
      "epoch": 1.4888507718696398,
      "grad_norm": 0.6581892967224121,
      "learning_rate": 5.112635791881075e-05,
      "loss": 0.4842,
      "step": 13020
    },
    {
      "epoch": 1.4911377930245855,
      "grad_norm": 0.7137598395347595,
      "learning_rate": 5.089765580331618e-05,
      "loss": 0.4696,
      "step": 13040
    },
    {
      "epoch": 1.4934248141795312,
      "grad_norm": 0.817304790019989,
      "learning_rate": 5.0668953687821616e-05,
      "loss": 0.4668,
      "step": 13060
    },
    {
      "epoch": 1.4957118353344767,
      "grad_norm": 0.5986533761024475,
      "learning_rate": 5.044025157232705e-05,
      "loss": 0.4931,
      "step": 13080
    },
    {
      "epoch": 1.4979988564894224,
      "grad_norm": 0.5487469434738159,
      "learning_rate": 5.0211549456832474e-05,
      "loss": 0.4319,
      "step": 13100
    },
    {
      "epoch": 1.5002858776443682,
      "grad_norm": 0.946566641330719,
      "learning_rate": 4.998284734133791e-05,
      "loss": 0.432,
      "step": 13120
    },
    {
      "epoch": 1.5025728987993139,
      "grad_norm": 1.2515887022018433,
      "learning_rate": 4.975414522584334e-05,
      "loss": 0.4477,
      "step": 13140
    },
    {
      "epoch": 1.5048599199542596,
      "grad_norm": 0.8172203898429871,
      "learning_rate": 4.952544311034877e-05,
      "loss": 0.4697,
      "step": 13160
    },
    {
      "epoch": 1.5071469411092053,
      "grad_norm": 0.7132264375686646,
      "learning_rate": 4.9296740994854204e-05,
      "loss": 0.4493,
      "step": 13180
    },
    {
      "epoch": 1.509433962264151,
      "grad_norm": 0.8408237099647522,
      "learning_rate": 4.906803887935964e-05,
      "loss": 0.4366,
      "step": 13200
    },
    {
      "epoch": 1.5117209834190968,
      "grad_norm": 0.5502535104751587,
      "learning_rate": 4.883933676386507e-05,
      "loss": 0.471,
      "step": 13220
    },
    {
      "epoch": 1.5140080045740423,
      "grad_norm": 0.5313640832901001,
      "learning_rate": 4.86106346483705e-05,
      "loss": 0.4784,
      "step": 13240
    },
    {
      "epoch": 1.516295025728988,
      "grad_norm": 0.6842594146728516,
      "learning_rate": 4.838193253287593e-05,
      "loss": 0.4628,
      "step": 13260
    },
    {
      "epoch": 1.5185820468839337,
      "grad_norm": 0.7711467146873474,
      "learning_rate": 4.815323041738136e-05,
      "loss": 0.4531,
      "step": 13280
    },
    {
      "epoch": 1.5208690680388792,
      "grad_norm": 0.5363322496414185,
      "learning_rate": 4.792452830188679e-05,
      "loss": 0.4761,
      "step": 13300
    },
    {
      "epoch": 1.523156089193825,
      "grad_norm": 0.7161095142364502,
      "learning_rate": 4.7695826186392225e-05,
      "loss": 0.4721,
      "step": 13320
    },
    {
      "epoch": 1.5254431103487707,
      "grad_norm": 0.5540586709976196,
      "learning_rate": 4.746712407089766e-05,
      "loss": 0.4916,
      "step": 13340
    },
    {
      "epoch": 1.5277301315037164,
      "grad_norm": 0.861899197101593,
      "learning_rate": 4.723842195540309e-05,
      "loss": 0.4876,
      "step": 13360
    },
    {
      "epoch": 1.5300171526586621,
      "grad_norm": 0.6792000532150269,
      "learning_rate": 4.700971983990852e-05,
      "loss": 0.4332,
      "step": 13380
    },
    {
      "epoch": 1.5323041738136078,
      "grad_norm": 0.6485698819160461,
      "learning_rate": 4.6781017724413955e-05,
      "loss": 0.4678,
      "step": 13400
    },
    {
      "epoch": 1.5345911949685536,
      "grad_norm": 0.7000424265861511,
      "learning_rate": 4.655231560891938e-05,
      "loss": 0.4638,
      "step": 13420
    },
    {
      "epoch": 1.5368782161234993,
      "grad_norm": 0.7445647716522217,
      "learning_rate": 4.6323613493424814e-05,
      "loss": 0.4329,
      "step": 13440
    },
    {
      "epoch": 1.5391652372784448,
      "grad_norm": 0.7573099732398987,
      "learning_rate": 4.609491137793025e-05,
      "loss": 0.4809,
      "step": 13460
    },
    {
      "epoch": 1.5414522584333905,
      "grad_norm": 0.7476420402526855,
      "learning_rate": 4.586620926243568e-05,
      "loss": 0.4866,
      "step": 13480
    },
    {
      "epoch": 1.5437392795883362,
      "grad_norm": 0.5904114842414856,
      "learning_rate": 4.563750714694111e-05,
      "loss": 0.4819,
      "step": 13500
    },
    {
      "epoch": 1.5460263007432817,
      "grad_norm": 0.8622945547103882,
      "learning_rate": 4.5408805031446544e-05,
      "loss": 0.4526,
      "step": 13520
    },
    {
      "epoch": 1.5483133218982275,
      "grad_norm": 0.9481425881385803,
      "learning_rate": 4.5180102915951976e-05,
      "loss": 0.4991,
      "step": 13540
    },
    {
      "epoch": 1.5506003430531732,
      "grad_norm": 0.5096725821495056,
      "learning_rate": 4.49514008004574e-05,
      "loss": 0.4636,
      "step": 13560
    },
    {
      "epoch": 1.552887364208119,
      "grad_norm": 0.7317091822624207,
      "learning_rate": 4.472269868496284e-05,
      "loss": 0.4764,
      "step": 13580
    },
    {
      "epoch": 1.5551743853630646,
      "grad_norm": 0.551736056804657,
      "learning_rate": 4.4493996569468274e-05,
      "loss": 0.4703,
      "step": 13600
    },
    {
      "epoch": 1.5574614065180103,
      "grad_norm": 0.6419956684112549,
      "learning_rate": 4.42652944539737e-05,
      "loss": 0.4269,
      "step": 13620
    },
    {
      "epoch": 1.559748427672956,
      "grad_norm": 0.6284191608428955,
      "learning_rate": 4.403659233847913e-05,
      "loss": 0.4473,
      "step": 13640
    },
    {
      "epoch": 1.5620354488279018,
      "grad_norm": 0.4962277114391327,
      "learning_rate": 4.3807890222984565e-05,
      "loss": 0.4655,
      "step": 13660
    },
    {
      "epoch": 1.5643224699828473,
      "grad_norm": 0.8143870234489441,
      "learning_rate": 4.357918810749e-05,
      "loss": 0.4454,
      "step": 13680
    },
    {
      "epoch": 1.566609491137793,
      "grad_norm": 0.6195862889289856,
      "learning_rate": 4.335048599199543e-05,
      "loss": 0.44,
      "step": 13700
    },
    {
      "epoch": 1.5688965122927387,
      "grad_norm": 0.6658357977867126,
      "learning_rate": 4.312178387650086e-05,
      "loss": 0.4702,
      "step": 13720
    },
    {
      "epoch": 1.5711835334476842,
      "grad_norm": 0.6612294912338257,
      "learning_rate": 4.2893081761006295e-05,
      "loss": 0.427,
      "step": 13740
    },
    {
      "epoch": 1.57347055460263,
      "grad_norm": 0.4453886151313782,
      "learning_rate": 4.266437964551172e-05,
      "loss": 0.4457,
      "step": 13760
    },
    {
      "epoch": 1.5757575757575757,
      "grad_norm": 0.7715744376182556,
      "learning_rate": 4.243567753001715e-05,
      "loss": 0.4454,
      "step": 13780
    },
    {
      "epoch": 1.5780445969125214,
      "grad_norm": 0.5456276535987854,
      "learning_rate": 4.2206975414522586e-05,
      "loss": 0.444,
      "step": 13800
    },
    {
      "epoch": 1.5803316180674671,
      "grad_norm": 0.6749127507209778,
      "learning_rate": 4.197827329902802e-05,
      "loss": 0.4573,
      "step": 13820
    },
    {
      "epoch": 1.5826186392224129,
      "grad_norm": 0.7242656350135803,
      "learning_rate": 4.174957118353345e-05,
      "loss": 0.4425,
      "step": 13840
    },
    {
      "epoch": 1.5849056603773586,
      "grad_norm": 1.0839999914169312,
      "learning_rate": 4.152086906803888e-05,
      "loss": 0.4789,
      "step": 13860
    },
    {
      "epoch": 1.5871926815323043,
      "grad_norm": 2.1000826358795166,
      "learning_rate": 4.1292166952544316e-05,
      "loss": 0.4743,
      "step": 13880
    },
    {
      "epoch": 1.5894797026872498,
      "grad_norm": 0.8211569786071777,
      "learning_rate": 4.106346483704974e-05,
      "loss": 0.4578,
      "step": 13900
    },
    {
      "epoch": 1.5917667238421955,
      "grad_norm": 0.5418036580085754,
      "learning_rate": 4.0834762721555174e-05,
      "loss": 0.4859,
      "step": 13920
    },
    {
      "epoch": 1.5940537449971413,
      "grad_norm": 0.634570300579071,
      "learning_rate": 4.0606060606060606e-05,
      "loss": 0.4415,
      "step": 13940
    },
    {
      "epoch": 1.5963407661520868,
      "grad_norm": 0.571846067905426,
      "learning_rate": 4.037735849056604e-05,
      "loss": 0.4992,
      "step": 13960
    },
    {
      "epoch": 1.5986277873070325,
      "grad_norm": 0.7873669266700745,
      "learning_rate": 4.014865637507147e-05,
      "loss": 0.4603,
      "step": 13980
    },
    {
      "epoch": 1.6009148084619782,
      "grad_norm": 0.7954727411270142,
      "learning_rate": 3.9919954259576904e-05,
      "loss": 0.4445,
      "step": 14000
    },
    {
      "epoch": 1.603201829616924,
      "grad_norm": 0.7246547937393188,
      "learning_rate": 3.9691252144082337e-05,
      "loss": 0.453,
      "step": 14020
    },
    {
      "epoch": 1.6054888507718696,
      "grad_norm": 0.4933248460292816,
      "learning_rate": 3.946255002858776e-05,
      "loss": 0.4865,
      "step": 14040
    },
    {
      "epoch": 1.6077758719268154,
      "grad_norm": 0.782767117023468,
      "learning_rate": 3.9233847913093195e-05,
      "loss": 0.4673,
      "step": 14060
    },
    {
      "epoch": 1.610062893081761,
      "grad_norm": 0.8279523849487305,
      "learning_rate": 3.900514579759863e-05,
      "loss": 0.454,
      "step": 14080
    },
    {
      "epoch": 1.6123499142367068,
      "grad_norm": 0.8085357546806335,
      "learning_rate": 3.8776443682104067e-05,
      "loss": 0.4718,
      "step": 14100
    },
    {
      "epoch": 1.6146369353916525,
      "grad_norm": 0.7560261487960815,
      "learning_rate": 3.854774156660949e-05,
      "loss": 0.4212,
      "step": 14120
    },
    {
      "epoch": 1.616923956546598,
      "grad_norm": 0.7379432320594788,
      "learning_rate": 3.8319039451114925e-05,
      "loss": 0.4998,
      "step": 14140
    },
    {
      "epoch": 1.6192109777015438,
      "grad_norm": 0.8035797476768494,
      "learning_rate": 3.809033733562036e-05,
      "loss": 0.4629,
      "step": 14160
    },
    {
      "epoch": 1.6214979988564893,
      "grad_norm": 0.6761304140090942,
      "learning_rate": 3.786163522012579e-05,
      "loss": 0.4314,
      "step": 14180
    },
    {
      "epoch": 1.623785020011435,
      "grad_norm": 0.6675986647605896,
      "learning_rate": 3.7632933104631216e-05,
      "loss": 0.4681,
      "step": 14200
    },
    {
      "epoch": 1.6260720411663807,
      "grad_norm": 0.558512806892395,
      "learning_rate": 3.740423098913665e-05,
      "loss": 0.4307,
      "step": 14220
    },
    {
      "epoch": 1.6283590623213264,
      "grad_norm": 0.8179248571395874,
      "learning_rate": 3.717552887364209e-05,
      "loss": 0.4666,
      "step": 14240
    },
    {
      "epoch": 1.6306460834762722,
      "grad_norm": 0.8064173460006714,
      "learning_rate": 3.694682675814751e-05,
      "loss": 0.4353,
      "step": 14260
    },
    {
      "epoch": 1.6329331046312179,
      "grad_norm": 0.6516194939613342,
      "learning_rate": 3.6718124642652946e-05,
      "loss": 0.4118,
      "step": 14280
    },
    {
      "epoch": 1.6352201257861636,
      "grad_norm": 0.8891735076904297,
      "learning_rate": 3.648942252715838e-05,
      "loss": 0.4675,
      "step": 14300
    },
    {
      "epoch": 1.6375071469411093,
      "grad_norm": 0.8091703653335571,
      "learning_rate": 3.626072041166381e-05,
      "loss": 0.4804,
      "step": 14320
    },
    {
      "epoch": 1.639794168096055,
      "grad_norm": 0.7527806758880615,
      "learning_rate": 3.6032018296169237e-05,
      "loss": 0.4694,
      "step": 14340
    },
    {
      "epoch": 1.6420811892510006,
      "grad_norm": 0.5226196646690369,
      "learning_rate": 3.5803316180674676e-05,
      "loss": 0.4907,
      "step": 14360
    },
    {
      "epoch": 1.6443682104059463,
      "grad_norm": 0.6893242597579956,
      "learning_rate": 3.557461406518011e-05,
      "loss": 0.4188,
      "step": 14380
    },
    {
      "epoch": 1.6466552315608918,
      "grad_norm": 0.6455610394477844,
      "learning_rate": 3.5345911949685534e-05,
      "loss": 0.4614,
      "step": 14400
    },
    {
      "epoch": 1.6489422527158375,
      "grad_norm": 0.7138192057609558,
      "learning_rate": 3.511720983419097e-05,
      "loss": 0.3995,
      "step": 14420
    },
    {
      "epoch": 1.6512292738707832,
      "grad_norm": 0.7073172926902771,
      "learning_rate": 3.48885077186964e-05,
      "loss": 0.4793,
      "step": 14440
    },
    {
      "epoch": 1.653516295025729,
      "grad_norm": 0.9456437826156616,
      "learning_rate": 3.465980560320183e-05,
      "loss": 0.4765,
      "step": 14460
    },
    {
      "epoch": 1.6558033161806747,
      "grad_norm": 0.8196824193000793,
      "learning_rate": 3.4431103487707264e-05,
      "loss": 0.4194,
      "step": 14480
    },
    {
      "epoch": 1.6580903373356204,
      "grad_norm": 0.5850800275802612,
      "learning_rate": 3.42024013722127e-05,
      "loss": 0.439,
      "step": 14500
    },
    {
      "epoch": 1.6603773584905661,
      "grad_norm": 0.6051052808761597,
      "learning_rate": 3.397369925671813e-05,
      "loss": 0.4452,
      "step": 14520
    },
    {
      "epoch": 1.6626643796455118,
      "grad_norm": 0.5481356382369995,
      "learning_rate": 3.3744997141223555e-05,
      "loss": 0.4465,
      "step": 14540
    },
    {
      "epoch": 1.6649514008004576,
      "grad_norm": 0.8616530895233154,
      "learning_rate": 3.351629502572899e-05,
      "loss": 0.4654,
      "step": 14560
    },
    {
      "epoch": 1.667238421955403,
      "grad_norm": 0.6981229782104492,
      "learning_rate": 3.328759291023442e-05,
      "loss": 0.4237,
      "step": 14580
    },
    {
      "epoch": 1.6695254431103488,
      "grad_norm": 0.8653804659843445,
      "learning_rate": 3.305889079473985e-05,
      "loss": 0.4126,
      "step": 14600
    },
    {
      "epoch": 1.6718124642652945,
      "grad_norm": 0.7029757499694824,
      "learning_rate": 3.2830188679245285e-05,
      "loss": 0.4712,
      "step": 14620
    },
    {
      "epoch": 1.67409948542024,
      "grad_norm": 0.6527381539344788,
      "learning_rate": 3.260148656375072e-05,
      "loss": 0.4592,
      "step": 14640
    },
    {
      "epoch": 1.6763865065751857,
      "grad_norm": 0.839955747127533,
      "learning_rate": 3.237278444825615e-05,
      "loss": 0.5587,
      "step": 14660
    },
    {
      "epoch": 1.6786735277301315,
      "grad_norm": 0.8512342572212219,
      "learning_rate": 3.2144082332761576e-05,
      "loss": 0.5236,
      "step": 14680
    },
    {
      "epoch": 1.6809605488850772,
      "grad_norm": 0.6637787818908691,
      "learning_rate": 3.191538021726701e-05,
      "loss": 0.4956,
      "step": 14700
    },
    {
      "epoch": 1.683247570040023,
      "grad_norm": 0.6906456351280212,
      "learning_rate": 3.168667810177244e-05,
      "loss": 0.437,
      "step": 14720
    },
    {
      "epoch": 1.6855345911949686,
      "grad_norm": 0.5913099050521851,
      "learning_rate": 3.1457975986277873e-05,
      "loss": 0.4614,
      "step": 14740
    },
    {
      "epoch": 1.6878216123499143,
      "grad_norm": 0.5912814140319824,
      "learning_rate": 3.1229273870783306e-05,
      "loss": 0.4574,
      "step": 14760
    },
    {
      "epoch": 1.69010863350486,
      "grad_norm": 0.8029168844223022,
      "learning_rate": 3.100057175528874e-05,
      "loss": 0.4383,
      "step": 14780
    },
    {
      "epoch": 1.6923956546598056,
      "grad_norm": 0.711836576461792,
      "learning_rate": 3.077186963979417e-05,
      "loss": 0.4636,
      "step": 14800
    },
    {
      "epoch": 1.6946826758147513,
      "grad_norm": 0.6796754002571106,
      "learning_rate": 3.05431675242996e-05,
      "loss": 0.4394,
      "step": 14820
    },
    {
      "epoch": 1.696969696969697,
      "grad_norm": 0.5588063597679138,
      "learning_rate": 3.0314465408805033e-05,
      "loss": 0.4589,
      "step": 14840
    },
    {
      "epoch": 1.6992567181246425,
      "grad_norm": 0.6451928615570068,
      "learning_rate": 3.0085763293310465e-05,
      "loss": 0.4852,
      "step": 14860
    },
    {
      "epoch": 1.7015437392795882,
      "grad_norm": 0.6980684995651245,
      "learning_rate": 2.9857061177815898e-05,
      "loss": 0.4742,
      "step": 14880
    },
    {
      "epoch": 1.703830760434534,
      "grad_norm": 0.6444355845451355,
      "learning_rate": 2.9628359062321327e-05,
      "loss": 0.4405,
      "step": 14900
    },
    {
      "epoch": 1.7061177815894797,
      "grad_norm": 0.8120569586753845,
      "learning_rate": 2.939965694682676e-05,
      "loss": 0.4907,
      "step": 14920
    },
    {
      "epoch": 1.7084048027444254,
      "grad_norm": 0.6896937489509583,
      "learning_rate": 2.9170954831332192e-05,
      "loss": 0.4565,
      "step": 14940
    },
    {
      "epoch": 1.7106918238993711,
      "grad_norm": 0.8609178066253662,
      "learning_rate": 2.8942252715837624e-05,
      "loss": 0.4807,
      "step": 14960
    },
    {
      "epoch": 1.7129788450543169,
      "grad_norm": 0.5426551103591919,
      "learning_rate": 2.8713550600343054e-05,
      "loss": 0.46,
      "step": 14980
    },
    {
      "epoch": 1.7152658662092626,
      "grad_norm": 0.6174366474151611,
      "learning_rate": 2.8484848484848486e-05,
      "loss": 0.4772,
      "step": 15000
    },
    {
      "epoch": 1.717552887364208,
      "grad_norm": 0.8239683508872986,
      "learning_rate": 2.825614636935392e-05,
      "loss": 0.4529,
      "step": 15020
    },
    {
      "epoch": 1.7198399085191538,
      "grad_norm": 0.49120020866394043,
      "learning_rate": 2.8027444253859348e-05,
      "loss": 0.4609,
      "step": 15040
    },
    {
      "epoch": 1.7221269296740995,
      "grad_norm": 0.6406815052032471,
      "learning_rate": 2.779874213836478e-05,
      "loss": 0.4333,
      "step": 15060
    },
    {
      "epoch": 1.724413950829045,
      "grad_norm": 0.7843337655067444,
      "learning_rate": 2.7570040022870213e-05,
      "loss": 0.4944,
      "step": 15080
    },
    {
      "epoch": 1.7267009719839908,
      "grad_norm": 0.5847391486167908,
      "learning_rate": 2.7341337907375645e-05,
      "loss": 0.484,
      "step": 15100
    },
    {
      "epoch": 1.7289879931389365,
      "grad_norm": 0.7265695333480835,
      "learning_rate": 2.7112635791881075e-05,
      "loss": 0.4759,
      "step": 15120
    },
    {
      "epoch": 1.7312750142938822,
      "grad_norm": 0.7755791544914246,
      "learning_rate": 2.6883933676386507e-05,
      "loss": 0.4467,
      "step": 15140
    },
    {
      "epoch": 1.733562035448828,
      "grad_norm": 0.7071905136108398,
      "learning_rate": 2.665523156089194e-05,
      "loss": 0.4726,
      "step": 15160
    },
    {
      "epoch": 1.7358490566037736,
      "grad_norm": 0.6329677104949951,
      "learning_rate": 2.642652944539737e-05,
      "loss": 0.4806,
      "step": 15180
    },
    {
      "epoch": 1.7381360777587194,
      "grad_norm": 1.1438283920288086,
      "learning_rate": 2.61978273299028e-05,
      "loss": 0.434,
      "step": 15200
    },
    {
      "epoch": 1.740423098913665,
      "grad_norm": 0.8652446269989014,
      "learning_rate": 2.5969125214408234e-05,
      "loss": 0.4964,
      "step": 15220
    },
    {
      "epoch": 1.7427101200686106,
      "grad_norm": 0.5772044062614441,
      "learning_rate": 2.574042309891367e-05,
      "loss": 0.4147,
      "step": 15240
    },
    {
      "epoch": 1.7449971412235563,
      "grad_norm": 0.6970888376235962,
      "learning_rate": 2.5511720983419095e-05,
      "loss": 0.4736,
      "step": 15260
    },
    {
      "epoch": 1.747284162378502,
      "grad_norm": 0.7343006730079651,
      "learning_rate": 2.5283018867924528e-05,
      "loss": 0.4153,
      "step": 15280
    },
    {
      "epoch": 1.7495711835334475,
      "grad_norm": 0.6472289562225342,
      "learning_rate": 2.5054316752429964e-05,
      "loss": 0.4519,
      "step": 15300
    },
    {
      "epoch": 1.7518582046883933,
      "grad_norm": 0.7078790068626404,
      "learning_rate": 2.4825614636935393e-05,
      "loss": 0.4675,
      "step": 15320
    },
    {
      "epoch": 1.754145225843339,
      "grad_norm": 0.7897304892539978,
      "learning_rate": 2.4596912521440822e-05,
      "loss": 0.4208,
      "step": 15340
    },
    {
      "epoch": 1.7564322469982847,
      "grad_norm": 0.48904523253440857,
      "learning_rate": 2.4368210405946258e-05,
      "loss": 0.4642,
      "step": 15360
    },
    {
      "epoch": 1.7587192681532304,
      "grad_norm": 0.7085704207420349,
      "learning_rate": 2.4139508290451687e-05,
      "loss": 0.445,
      "step": 15380
    },
    {
      "epoch": 1.7610062893081762,
      "grad_norm": 0.759782612323761,
      "learning_rate": 2.391080617495712e-05,
      "loss": 0.4696,
      "step": 15400
    },
    {
      "epoch": 1.7632933104631219,
      "grad_norm": 0.7813954949378967,
      "learning_rate": 2.3682104059462552e-05,
      "loss": 0.4576,
      "step": 15420
    },
    {
      "epoch": 1.7655803316180676,
      "grad_norm": 0.9641011953353882,
      "learning_rate": 2.3453401943967985e-05,
      "loss": 0.4567,
      "step": 15440
    },
    {
      "epoch": 1.767867352773013,
      "grad_norm": 0.5476062893867493,
      "learning_rate": 2.3224699828473414e-05,
      "loss": 0.4686,
      "step": 15460
    },
    {
      "epoch": 1.7701543739279588,
      "grad_norm": 0.8316104412078857,
      "learning_rate": 2.2995997712978846e-05,
      "loss": 0.4601,
      "step": 15480
    },
    {
      "epoch": 1.7724413950829045,
      "grad_norm": 1.0022752285003662,
      "learning_rate": 2.276729559748428e-05,
      "loss": 0.527,
      "step": 15500
    },
    {
      "epoch": 1.77472841623785,
      "grad_norm": 0.5720147490501404,
      "learning_rate": 2.2538593481989708e-05,
      "loss": 0.4571,
      "step": 15520
    },
    {
      "epoch": 1.7770154373927958,
      "grad_norm": 0.6754027009010315,
      "learning_rate": 2.230989136649514e-05,
      "loss": 0.5092,
      "step": 15540
    },
    {
      "epoch": 1.7793024585477415,
      "grad_norm": 0.7321606278419495,
      "learning_rate": 2.2081189251000573e-05,
      "loss": 0.4683,
      "step": 15560
    },
    {
      "epoch": 1.7815894797026872,
      "grad_norm": 0.8884972929954529,
      "learning_rate": 2.1852487135506006e-05,
      "loss": 0.48,
      "step": 15580
    },
    {
      "epoch": 1.783876500857633,
      "grad_norm": 0.7659553289413452,
      "learning_rate": 2.1623785020011435e-05,
      "loss": 0.4399,
      "step": 15600
    },
    {
      "epoch": 1.7861635220125787,
      "grad_norm": 0.8052449822425842,
      "learning_rate": 2.1395082904516867e-05,
      "loss": 0.4602,
      "step": 15620
    },
    {
      "epoch": 1.7884505431675244,
      "grad_norm": 0.8966054916381836,
      "learning_rate": 2.11663807890223e-05,
      "loss": 0.4934,
      "step": 15640
    },
    {
      "epoch": 1.79073756432247,
      "grad_norm": 0.49429774284362793,
      "learning_rate": 2.093767867352773e-05,
      "loss": 0.4561,
      "step": 15660
    },
    {
      "epoch": 1.7930245854774156,
      "grad_norm": 0.5992509126663208,
      "learning_rate": 2.0708976558033165e-05,
      "loss": 0.4747,
      "step": 15680
    },
    {
      "epoch": 1.7953116066323613,
      "grad_norm": 0.7726767659187317,
      "learning_rate": 2.0480274442538594e-05,
      "loss": 0.491,
      "step": 15700
    },
    {
      "epoch": 1.797598627787307,
      "grad_norm": 0.6720061898231506,
      "learning_rate": 2.0251572327044027e-05,
      "loss": 0.424,
      "step": 15720
    },
    {
      "epoch": 1.7998856489422526,
      "grad_norm": 0.6369178295135498,
      "learning_rate": 2.002287021154946e-05,
      "loss": 0.4585,
      "step": 15740
    },
    {
      "epoch": 1.8021726700971983,
      "grad_norm": 0.7474898099899292,
      "learning_rate": 1.9794168096054888e-05,
      "loss": 0.4396,
      "step": 15760
    },
    {
      "epoch": 1.804459691252144,
      "grad_norm": 0.662970781326294,
      "learning_rate": 1.956546598056032e-05,
      "loss": 0.4628,
      "step": 15780
    },
    {
      "epoch": 1.8067467124070897,
      "grad_norm": 0.6192646026611328,
      "learning_rate": 1.9336763865065753e-05,
      "loss": 0.4772,
      "step": 15800
    },
    {
      "epoch": 1.8090337335620355,
      "grad_norm": 0.7525639533996582,
      "learning_rate": 1.9108061749571186e-05,
      "loss": 0.4601,
      "step": 15820
    },
    {
      "epoch": 1.8113207547169812,
      "grad_norm": 0.652213990688324,
      "learning_rate": 1.8879359634076615e-05,
      "loss": 0.4726,
      "step": 15840
    },
    {
      "epoch": 1.813607775871927,
      "grad_norm": 0.5550232529640198,
      "learning_rate": 1.8650657518582047e-05,
      "loss": 0.4135,
      "step": 15860
    },
    {
      "epoch": 1.8158947970268726,
      "grad_norm": 0.5532954335212708,
      "learning_rate": 1.842195540308748e-05,
      "loss": 0.4737,
      "step": 15880
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 0.7129321694374084,
      "learning_rate": 1.8193253287592912e-05,
      "loss": 0.4984,
      "step": 15900
    },
    {
      "epoch": 1.8204688393367638,
      "grad_norm": 0.6369949579238892,
      "learning_rate": 1.796455117209834e-05,
      "loss": 0.4414,
      "step": 15920
    },
    {
      "epoch": 1.8227558604917096,
      "grad_norm": 0.6367284655570984,
      "learning_rate": 1.7735849056603774e-05,
      "loss": 0.4736,
      "step": 15940
    },
    {
      "epoch": 1.825042881646655,
      "grad_norm": 0.9052444100379944,
      "learning_rate": 1.7507146941109207e-05,
      "loss": 0.5271,
      "step": 15960
    },
    {
      "epoch": 1.8273299028016008,
      "grad_norm": 0.6251180171966553,
      "learning_rate": 1.7278444825614636e-05,
      "loss": 0.4813,
      "step": 15980
    },
    {
      "epoch": 1.8296169239565465,
      "grad_norm": 0.7082495093345642,
      "learning_rate": 1.7049742710120072e-05,
      "loss": 0.4454,
      "step": 16000
    },
    {
      "epoch": 1.8319039451114922,
      "grad_norm": 0.7210286855697632,
      "learning_rate": 1.68210405946255e-05,
      "loss": 0.4766,
      "step": 16020
    },
    {
      "epoch": 1.834190966266438,
      "grad_norm": 0.6637722849845886,
      "learning_rate": 1.6592338479130933e-05,
      "loss": 0.5076,
      "step": 16040
    },
    {
      "epoch": 1.8364779874213837,
      "grad_norm": 0.909565806388855,
      "learning_rate": 1.6363636363636366e-05,
      "loss": 0.4901,
      "step": 16060
    },
    {
      "epoch": 1.8387650085763294,
      "grad_norm": 0.5885821580886841,
      "learning_rate": 1.6134934248141795e-05,
      "loss": 0.4563,
      "step": 16080
    },
    {
      "epoch": 1.8410520297312751,
      "grad_norm": 0.8413470983505249,
      "learning_rate": 1.5906232132647228e-05,
      "loss": 0.482,
      "step": 16100
    },
    {
      "epoch": 1.8433390508862209,
      "grad_norm": 0.7297347187995911,
      "learning_rate": 1.567753001715266e-05,
      "loss": 0.475,
      "step": 16120
    },
    {
      "epoch": 1.8456260720411664,
      "grad_norm": 0.6660379767417908,
      "learning_rate": 1.5448827901658093e-05,
      "loss": 0.4936,
      "step": 16140
    },
    {
      "epoch": 1.847913093196112,
      "grad_norm": 0.5759689211845398,
      "learning_rate": 1.5220125786163522e-05,
      "loss": 0.43,
      "step": 16160
    },
    {
      "epoch": 1.8502001143510578,
      "grad_norm": 0.7336426973342896,
      "learning_rate": 1.4991423670668956e-05,
      "loss": 0.4604,
      "step": 16180
    },
    {
      "epoch": 1.8524871355060033,
      "grad_norm": 0.5962771773338318,
      "learning_rate": 1.4762721555174385e-05,
      "loss": 0.4881,
      "step": 16200
    },
    {
      "epoch": 1.854774156660949,
      "grad_norm": 0.6663607954978943,
      "learning_rate": 1.453401943967982e-05,
      "loss": 0.476,
      "step": 16220
    },
    {
      "epoch": 1.8570611778158947,
      "grad_norm": 0.5990496277809143,
      "learning_rate": 1.430531732418525e-05,
      "loss": 0.4743,
      "step": 16240
    },
    {
      "epoch": 1.8593481989708405,
      "grad_norm": 0.6583651304244995,
      "learning_rate": 1.407661520869068e-05,
      "loss": 0.4628,
      "step": 16260
    },
    {
      "epoch": 1.8616352201257862,
      "grad_norm": 0.49309590458869934,
      "learning_rate": 1.3847913093196114e-05,
      "loss": 0.4356,
      "step": 16280
    },
    {
      "epoch": 1.863922241280732,
      "grad_norm": 0.9026497602462769,
      "learning_rate": 1.3619210977701544e-05,
      "loss": 0.4468,
      "step": 16300
    },
    {
      "epoch": 1.8662092624356776,
      "grad_norm": 0.7842590808868408,
      "learning_rate": 1.3390508862206977e-05,
      "loss": 0.4437,
      "step": 16320
    },
    {
      "epoch": 1.8684962835906234,
      "grad_norm": 0.6681782007217407,
      "learning_rate": 1.3161806746712408e-05,
      "loss": 0.408,
      "step": 16340
    },
    {
      "epoch": 1.8707833047455689,
      "grad_norm": 0.8640637993812561,
      "learning_rate": 1.293310463121784e-05,
      "loss": 0.4907,
      "step": 16360
    },
    {
      "epoch": 1.8730703259005146,
      "grad_norm": 1.1858329772949219,
      "learning_rate": 1.2704402515723271e-05,
      "loss": 0.4449,
      "step": 16380
    },
    {
      "epoch": 1.8753573470554603,
      "grad_norm": 0.8249523043632507,
      "learning_rate": 1.2475700400228704e-05,
      "loss": 0.43,
      "step": 16400
    },
    {
      "epoch": 1.8776443682104058,
      "grad_norm": 0.7887488603591919,
      "learning_rate": 1.2246998284734134e-05,
      "loss": 0.4772,
      "step": 16420
    },
    {
      "epoch": 1.8799313893653515,
      "grad_norm": 0.5989006161689758,
      "learning_rate": 1.2018296169239567e-05,
      "loss": 0.4316,
      "step": 16440
    },
    {
      "epoch": 1.8822184105202973,
      "grad_norm": 0.6245386600494385,
      "learning_rate": 1.1789594053744998e-05,
      "loss": 0.4485,
      "step": 16460
    },
    {
      "epoch": 1.884505431675243,
      "grad_norm": 0.7019944190979004,
      "learning_rate": 1.1560891938250429e-05,
      "loss": 0.494,
      "step": 16480
    },
    {
      "epoch": 1.8867924528301887,
      "grad_norm": 0.6590589284896851,
      "learning_rate": 1.1332189822755861e-05,
      "loss": 0.4466,
      "step": 16500
    },
    {
      "epoch": 1.8890794739851344,
      "grad_norm": 0.7422148585319519,
      "learning_rate": 1.1103487707261292e-05,
      "loss": 0.4479,
      "step": 16520
    },
    {
      "epoch": 1.8913664951400802,
      "grad_norm": 0.9436569809913635,
      "learning_rate": 1.0874785591766724e-05,
      "loss": 0.4454,
      "step": 16540
    },
    {
      "epoch": 1.8936535162950259,
      "grad_norm": 0.8123020529747009,
      "learning_rate": 1.0646083476272157e-05,
      "loss": 0.4521,
      "step": 16560
    },
    {
      "epoch": 1.8959405374499714,
      "grad_norm": 0.5966178178787231,
      "learning_rate": 1.0417381360777588e-05,
      "loss": 0.4435,
      "step": 16580
    },
    {
      "epoch": 1.898227558604917,
      "grad_norm": 0.6280995607376099,
      "learning_rate": 1.018867924528302e-05,
      "loss": 0.4759,
      "step": 16600
    },
    {
      "epoch": 1.9005145797598628,
      "grad_norm": 0.7596681714057922,
      "learning_rate": 9.959977129788451e-06,
      "loss": 0.469,
      "step": 16620
    },
    {
      "epoch": 1.9028016009148083,
      "grad_norm": 0.6969398856163025,
      "learning_rate": 9.731275014293882e-06,
      "loss": 0.4778,
      "step": 16640
    },
    {
      "epoch": 1.905088622069754,
      "grad_norm": 0.7421548366546631,
      "learning_rate": 9.502572898799315e-06,
      "loss": 0.4466,
      "step": 16660
    },
    {
      "epoch": 1.9073756432246998,
      "grad_norm": 0.6281481385231018,
      "learning_rate": 9.273870783304745e-06,
      "loss": 0.397,
      "step": 16680
    },
    {
      "epoch": 1.9096626643796455,
      "grad_norm": 0.3210742771625519,
      "learning_rate": 9.045168667810178e-06,
      "loss": 0.4547,
      "step": 16700
    },
    {
      "epoch": 1.9119496855345912,
      "grad_norm": 0.9153731465339661,
      "learning_rate": 8.816466552315609e-06,
      "loss": 0.4452,
      "step": 16720
    },
    {
      "epoch": 1.914236706689537,
      "grad_norm": 1.1011295318603516,
      "learning_rate": 8.587764436821041e-06,
      "loss": 0.4402,
      "step": 16740
    },
    {
      "epoch": 1.9165237278444827,
      "grad_norm": 0.8724344372749329,
      "learning_rate": 8.359062321326474e-06,
      "loss": 0.4947,
      "step": 16760
    },
    {
      "epoch": 1.9188107489994284,
      "grad_norm": 0.65571129322052,
      "learning_rate": 8.130360205831903e-06,
      "loss": 0.4757,
      "step": 16780
    },
    {
      "epoch": 1.9210977701543739,
      "grad_norm": 0.7402104139328003,
      "learning_rate": 7.901658090337335e-06,
      "loss": 0.4581,
      "step": 16800
    },
    {
      "epoch": 1.9233847913093196,
      "grad_norm": 0.8161584138870239,
      "learning_rate": 7.672955974842768e-06,
      "loss": 0.4398,
      "step": 16820
    },
    {
      "epoch": 1.9256718124642653,
      "grad_norm": 0.9884721040725708,
      "learning_rate": 7.444253859348199e-06,
      "loss": 0.4646,
      "step": 16840
    },
    {
      "epoch": 1.9279588336192108,
      "grad_norm": 0.796510636806488,
      "learning_rate": 7.215551743853631e-06,
      "loss": 0.4724,
      "step": 16860
    },
    {
      "epoch": 1.9302458547741566,
      "grad_norm": 0.9979209899902344,
      "learning_rate": 6.986849628359063e-06,
      "loss": 0.4745,
      "step": 16880
    },
    {
      "epoch": 1.9325328759291023,
      "grad_norm": 0.6828279495239258,
      "learning_rate": 6.758147512864495e-06,
      "loss": 0.4676,
      "step": 16900
    },
    {
      "epoch": 1.934819897084048,
      "grad_norm": 0.5129024982452393,
      "learning_rate": 6.529445397369926e-06,
      "loss": 0.4495,
      "step": 16920
    },
    {
      "epoch": 1.9371069182389937,
      "grad_norm": 0.6245166659355164,
      "learning_rate": 6.300743281875357e-06,
      "loss": 0.4252,
      "step": 16940
    },
    {
      "epoch": 1.9393939393939394,
      "grad_norm": 0.7303065657615662,
      "learning_rate": 6.07204116638079e-06,
      "loss": 0.4587,
      "step": 16960
    },
    {
      "epoch": 1.9416809605488852,
      "grad_norm": 0.6457876563072205,
      "learning_rate": 5.8433390508862205e-06,
      "loss": 0.4641,
      "step": 16980
    },
    {
      "epoch": 1.943967981703831,
      "grad_norm": 0.8825851678848267,
      "learning_rate": 5.614636935391652e-06,
      "loss": 0.4629,
      "step": 17000
    },
    {
      "epoch": 1.9462550028587764,
      "grad_norm": 0.4763181209564209,
      "learning_rate": 5.385934819897085e-06,
      "loss": 0.4515,
      "step": 17020
    },
    {
      "epoch": 1.9485420240137221,
      "grad_norm": 0.6714112162590027,
      "learning_rate": 5.157232704402516e-06,
      "loss": 0.4398,
      "step": 17040
    },
    {
      "epoch": 1.9508290451686678,
      "grad_norm": 0.945085883140564,
      "learning_rate": 4.928530588907947e-06,
      "loss": 0.4528,
      "step": 17060
    },
    {
      "epoch": 1.9531160663236133,
      "grad_norm": 0.5718001127243042,
      "learning_rate": 4.699828473413379e-06,
      "loss": 0.4425,
      "step": 17080
    },
    {
      "epoch": 1.955403087478559,
      "grad_norm": 0.9571990966796875,
      "learning_rate": 4.4711263579188114e-06,
      "loss": 0.4485,
      "step": 17100
    },
    {
      "epoch": 1.9576901086335048,
      "grad_norm": 0.6839303970336914,
      "learning_rate": 4.242424242424243e-06,
      "loss": 0.4383,
      "step": 17120
    },
    {
      "epoch": 1.9599771297884505,
      "grad_norm": 0.919394314289093,
      "learning_rate": 4.013722126929674e-06,
      "loss": 0.4939,
      "step": 17140
    },
    {
      "epoch": 1.9622641509433962,
      "grad_norm": 0.6136085391044617,
      "learning_rate": 3.785020011435106e-06,
      "loss": 0.4075,
      "step": 17160
    },
    {
      "epoch": 1.964551172098342,
      "grad_norm": 0.513446569442749,
      "learning_rate": 3.5563178959405377e-06,
      "loss": 0.4762,
      "step": 17180
    },
    {
      "epoch": 1.9668381932532877,
      "grad_norm": 0.7993113994598389,
      "learning_rate": 3.3276157804459694e-06,
      "loss": 0.4283,
      "step": 17200
    },
    {
      "epoch": 1.9691252144082334,
      "grad_norm": 0.6034188270568848,
      "learning_rate": 3.098913664951401e-06,
      "loss": 0.4949,
      "step": 17220
    },
    {
      "epoch": 1.971412235563179,
      "grad_norm": 0.7569889426231384,
      "learning_rate": 2.8702115494568327e-06,
      "loss": 0.4511,
      "step": 17240
    },
    {
      "epoch": 1.9736992567181246,
      "grad_norm": 1.0688780546188354,
      "learning_rate": 2.6415094339622644e-06,
      "loss": 0.4386,
      "step": 17260
    },
    {
      "epoch": 1.9759862778730704,
      "grad_norm": 0.5972617864608765,
      "learning_rate": 2.4128073184676957e-06,
      "loss": 0.484,
      "step": 17280
    },
    {
      "epoch": 1.9782732990280159,
      "grad_norm": 0.7152014374732971,
      "learning_rate": 2.1841052029731278e-06,
      "loss": 0.3858,
      "step": 17300
    },
    {
      "epoch": 1.9805603201829616,
      "grad_norm": 0.7971901893615723,
      "learning_rate": 1.955403087478559e-06,
      "loss": 0.5074,
      "step": 17320
    },
    {
      "epoch": 1.9828473413379073,
      "grad_norm": 0.8264644742012024,
      "learning_rate": 1.7267009719839911e-06,
      "loss": 0.4481,
      "step": 17340
    },
    {
      "epoch": 1.985134362492853,
      "grad_norm": 0.7133603096008301,
      "learning_rate": 1.4979988564894226e-06,
      "loss": 0.4509,
      "step": 17360
    },
    {
      "epoch": 1.9874213836477987,
      "grad_norm": 0.5764490365982056,
      "learning_rate": 1.2692967409948543e-06,
      "loss": 0.4967,
      "step": 17380
    },
    {
      "epoch": 1.9897084048027445,
      "grad_norm": 0.6604889631271362,
      "learning_rate": 1.040594625500286e-06,
      "loss": 0.4491,
      "step": 17400
    },
    {
      "epoch": 1.9919954259576902,
      "grad_norm": 1.0574513673782349,
      "learning_rate": 8.118925100057175e-07,
      "loss": 0.4414,
      "step": 17420
    },
    {
      "epoch": 1.994282447112636,
      "grad_norm": 0.7385045289993286,
      "learning_rate": 5.831903945111492e-07,
      "loss": 0.4534,
      "step": 17440
    },
    {
      "epoch": 1.9965694682675816,
      "grad_norm": 0.7316728234291077,
      "learning_rate": 3.5448827901658093e-07,
      "loss": 0.4839,
      "step": 17460
    },
    {
      "epoch": 1.9988564894225271,
      "grad_norm": 0.5492677092552185,
      "learning_rate": 1.2578616352201258e-07,
      "loss": 0.4541,
      "step": 17480
    }
  ],
  "logging_steps": 20,
  "max_steps": 17490,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5.22308277485568e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
